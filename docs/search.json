[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dlfc_notes",
    "section": "",
    "text": "Preface\nThis project is a WIP.\nI’m a great fan of Bishop’s 2006 book, Pattern Recognition and Machine Learning (PRML), so as an AI practioner coming from a Bayesian background, I’m very much pleasantly surprised to learn that Bishop (and Bishop) have published a new book dedicated to deep learning and artificial intelligence. The focuse on generative models is especially enticing, firstly because it is all the rage for the moment; secondly for a Bayesian statistician, doing generative modeling has always been what we are trained for.\nThe book has 20 chapters, I have divided them roughly into three parts.\n\nFoundations: chapters 1-5, covers the basics of machine learning.\nDeep Learning: chapters 6-13, covers the nuts and bolts of deep learning.\nGenerative Models: chapters 14-20, covers generative modeling and some common model architectures.\n\nHerein collected are my notes on the book, and the code to implement the examples in the book. Since these are my understanding of the book, they might not be completely accurate, so please refer to the book for the authoritative source. And if you find any errors, do let me know!\nThe notes and code are written in a literate programming style (See Knuth (1984) for additional discussion). The references are not complete; I only included the ones I have read.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  The Deep Learning Revolution",
    "section": "",
    "text": "1.1 the impact of DL (4)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#the-impact-of-dl-4",
    "href": "01.html#the-impact-of-dl-4",
    "title": "1  The Deep Learning Revolution",
    "section": "",
    "text": "1.1.1 midical diagnosis\n\n\n\n1.1.2 protein structure\n\n\n\n1.1.3 image synthesis\n\n\n\n1.1.4 LLM",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#a-tutorial-example-6",
    "href": "01.html#a-tutorial-example-6",
    "title": "1  The Deep Learning Revolution",
    "section": "1.2 a tutorial example (6)",
    "text": "1.2 a tutorial example (6)\n\n1.2.1 synthetic data\n\n\n\n1.2.2 linear models\n\n\n1.2.3 error function\n\n\n\n1.2.4 model complexity\n      \n\n\n1.2.5 regularization\n  \n\n\n1.2.6 model selection",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#a-brief-history-of-ml",
    "href": "01.html#a-brief-history-of-ml",
    "title": "1  The Deep Learning Revolution",
    "section": "1.3 a brief history of ML",
    "text": "1.3 a brief history of ML\n\n\n1.3.1 single layer networks\n\n\n\n1.3.2 backpropagation\n\n\n\n1.3.3 deep networks",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "18.html",
    "href": "18.html",
    "title": "2  Normalizing Flows",
    "section": "",
    "text": "2.1 coupling flows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#neural-differential-equation",
    "href": "18.html#neural-differential-equation",
    "title": "2  Normalizing Flows",
    "section": "5.1 neural differential equation",
    "text": "5.1 neural differential equation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#neural-ode-backpropagation",
    "href": "18.html#neural-ode-backpropagation",
    "title": "2  Normalizing Flows",
    "section": "5.2 neural ODE backpropagation",
    "text": "5.2 neural ODE backpropagation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#neural-ode-flows",
    "href": "18.html#neural-ode-flows",
    "title": "2  Normalizing Flows",
    "section": "5.3 neural ODE flows",
    "text": "5.3 neural ODE flows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "01.html#midical-diagnosis",
    "href": "01.html#midical-diagnosis",
    "title": "1  The Deep Learning Revolution",
    "section": "2.1 midical diagnosis",
    "text": "2.1 midical diagnosis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#protein-structure",
    "href": "01.html#protein-structure",
    "title": "1  The Deep Learning Revolution",
    "section": "2.2 protein structure",
    "text": "2.2 protein structure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#image-synthesis",
    "href": "01.html#image-synthesis",
    "title": "1  The Deep Learning Revolution",
    "section": "2.3 image synthesis",
    "text": "2.3 image synthesis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#llm",
    "href": "01.html#llm",
    "title": "1  The Deep Learning Revolution",
    "section": "2.4 LLM",
    "text": "2.4 LLM",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#synthetic-data",
    "href": "01.html#synthetic-data",
    "title": "1  The Deep Learning Revolution",
    "section": "3.1 synthetic data",
    "text": "3.1 synthetic data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#linear-models",
    "href": "01.html#linear-models",
    "title": "1  The Deep Learning Revolution",
    "section": "3.2 linear models",
    "text": "3.2 linear models",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#error-function",
    "href": "01.html#error-function",
    "title": "1  The Deep Learning Revolution",
    "section": "3.3 error function",
    "text": "3.3 error function",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#model-complexity",
    "href": "01.html#model-complexity",
    "title": "1  The Deep Learning Revolution",
    "section": "3.4 model complexity",
    "text": "3.4 model complexity",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#regularization",
    "href": "01.html#regularization",
    "title": "1  The Deep Learning Revolution",
    "section": "3.5 regularization",
    "text": "3.5 regularization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#model-selection",
    "href": "01.html#model-selection",
    "title": "1  The Deep Learning Revolution",
    "section": "3.6 model selection",
    "text": "3.6 model selection",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#single-layer-networks",
    "href": "01.html#single-layer-networks",
    "title": "1  The Deep Learning Revolution",
    "section": "4.1 single layer networks",
    "text": "4.1 single layer networks",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#backpropagation",
    "href": "01.html#backpropagation",
    "title": "1  The Deep Learning Revolution",
    "section": "4.2 backpropagation",
    "text": "4.2 backpropagation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#deep-networks",
    "href": "01.html#deep-networks",
    "title": "1  The Deep Learning Revolution",
    "section": "4.3 deep networks",
    "text": "4.3 deep networks",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "18.html#autoregressive-flows",
    "href": "18.html#autoregressive-flows",
    "title": "2  Normalizing Flows",
    "section": "2.2 autoregressive flows",
    "text": "2.2 autoregressive flows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#continuous-flows-3",
    "href": "18.html#continuous-flows-3",
    "title": "2  Normalizing Flows",
    "section": "2.3 continuous flows (3)",
    "text": "2.3 continuous flows (3)\nContinuous flows, particularly those based on Neural Ordinary Differential Equations (Neural ODEs), offer a sophisticated framework for modeling transformations of continuous state spaces. Unlike discrete flows that apply transformations in layers or steps, continuous flows define the transformation through the solution of a differential equation over a continuous depth or time variable.\nNeural ODEs conceptualize the layers of a deep network as a continuous dynamic process governed by an ordinary differential equation. For an input $ $, the output $ $ is obtained by solving the following ODE:\n\\[\n\\frac{d\\mathbf{z}(t)}{dt} = f(\\mathbf{z}(t), t, \\mathbf{\\theta})\n\\]\nWhere:\n\n$ (t) $ represents the state of the system at time $ t $.\n$ f $ is a neural network parametrized by weights $ $.\n$ (0) = $ and $ (1) = $ (assuming integration from $ t=0 $ to $ t=1 $).\n\nThe transformation $ = (1) $ is defined implicitly by the trajectory that $ (t) $ follows, dictated by $ f \\(.\nThis setup allows an infinitely deep network by parameterizing the\ncontinuous transformation depth with a fixed set of parameters\\) $.\nTraining Neural ODEs involves backpropagation through the solution of the ODE. The adjoint sensitivity method is the standard approach, which computes gradients without retaining the entire trajectory of $ (t) $. This is formulated as:\n\\[\n\\frac{d\\mathbf{a}(t)}{dt} = -\\mathbf{a}(t)^T \\frac{\\partial f}{\\partial \\mathbf{z}(t)}\n\\]\nWhere $ (t) $ is the adjoint state, representing the gradient of the loss $ $ with respect to $ (t) \\(. This\nmethod efficiently computes gradients by solving a coupled ODE backwards\nin time, starting from\\) (1) = $.\nNeural ODE Flows extend the concept of Neural ODEs to model complex distributions by transforming a simple base distribution (like a standard Gaussian) into a more complex one through the flow defined by the ODE. The key benefit here is the computation of the log-likelihood adjustment due to the change of variables formula, which involves the trace of the Jacobian rather than its determinant:\n\\[\n\\text{Trace} \\left(\\frac{\\partial f}{\\partial \\mathbf{z}(t)}\\right)\n\\]\nThis trace can be estimated stochastically (e.g., Hutchinson's trace estimator), making the model both memory efficient and scalable for high-dimensional data.\nThese continuous and neural ODE-based flows provide a powerful framework for deep generative models, especially in scenarios where the data or the model's behavior is inherently continuous.\n\n2.3.1 neural differential equation\n\n\n2.3.2 neural ODE backpropagation\n\n\n2.3.3 neural ODE flows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#continuous-flows",
    "href": "18.html#continuous-flows",
    "title": "2  Normalizing Flows",
    "section": "2.3 continuous flows",
    "text": "2.3 continuous flows\n\n2.3.1 neural differential equation\n\n\n2.3.2 neural ODE backpropagation\n\n\n2.3.3 neural ODE flows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  }
]