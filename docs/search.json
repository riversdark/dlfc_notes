[
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  The Deep Learning Revolution",
    "section": "",
    "text": "1.1 the impact of DL (4)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#the-impact-of-dl-4",
    "href": "01.html#the-impact-of-dl-4",
    "title": "1  The Deep Learning Revolution",
    "section": "",
    "text": "1.1.1 midical diagnosis\n\n\n\n1.1.2 protein structure\n\n\n\n1.1.3 image synthesis\n\n\n\n1.1.4 LLM",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#a-tutorial-example-6",
    "href": "01.html#a-tutorial-example-6",
    "title": "1  The Deep Learning Revolution",
    "section": "1.2 a tutorial example (6)",
    "text": "1.2 a tutorial example (6)\n\n1.2.1 synthetic data\n\n\n\n1.2.2 linear models\n\n\n1.2.3 error function\n\n\n\n1.2.4 model complexity\n      \n\n\n1.2.5 regularization\n  \n\n\n1.2.6 model selection",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "01.html#a-brief-history-of-ml",
    "href": "01.html#a-brief-history-of-ml",
    "title": "1  The Deep Learning Revolution",
    "section": "1.3 a brief history of ML",
    "text": "1.3 a brief history of ML\n\n\n1.3.1 single layer networks\n\n\n\n1.3.2 backpropagation\n\n\n\n1.3.3 deep networks",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Deep Learning Revolution</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dlfc_notes",
    "section": "",
    "text": "Preface\nThis project is a WIP.\nI’m a great fan of Bishop’s 2006 book, Pattern Recognition and Machine Learning (PRML), so as an AI practioner coming from a Bayesian background, I’m quite happy to learn that Bishop (and Bishop) have published a new book dedicated to deep learning and artificial intelligence. The focus on generative models is especially enticing, firstly because it is all the rage for the moment; secondly for a Bayesian statistician, doing generative modeling has always been what we are trained for.\nThe book has 20 chapters, I have divided them roughly into three parts.\n\nFoundations: chapters 1-5, covers the basics of machine learning.\nDeep Learning: chapters 6-13, covers the nuts and bolts of deep learning.\nGenerative Models: chapters 14-20, covers generative modeling and some common model architectures.\n\nHerein collected are my notes of the book, and the code to implement some concepts from the book. Since these are my understanding of the book, they might not be completely accurate, so do refer to the book for the most accurate description. And let me know if you find any errors!\nThe notes and code are written in a literate programming style (See Knuth (1984) for additional discussion). The references are not complete; I only included the ones I have reviewed myself.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "18.html",
    "href": "18.html",
    "title": "3  Normalizing Flows",
    "section": "",
    "text": "3.1 coupling flows\nRecall that in flow models we aim for the following goals:\nEach flow model will attempt to meet these goals in different ways. In coupling flows, for each layer of the network, we first split the latent variable \\(\\mathbf{z}\\) into two parts \\(\\mathbf{z} = (\\mathbf{z}_A, \\mathbf{z}_B)\\), then apply the following transformation:\n\\[\n\\begin{align}\n\\mathbf{X}_A &= \\mathbf{Z}_A, \\\\\n\\mathbf{X}_B &= \\exp(s(\\mathbf{Z}_A, w)) \\odot \\mathbf{Z}_B + b(\\mathbf{Z}_A, w).\n\\end{align}\n\\]\nThe frist part \\(\\mathbf{X}_A\\) is simply left unchanged, and all the efforts are put into transforming the second part \\(\\mathbf{X}_B\\). The transformation is done by a neural network with parameters \\(w\\), which takes \\(\\mathbf{Z}_A\\) as input and outputs two vectors \\(s(\\mathbf{Z}_A, w)\\) and \\(b(\\mathbf{Z}_A, w)\\) of the same dimensionality as \\(\\mathbf{Z}_B\\). Besides, an \\(\\exp\\) function is used to ensure that the Jacobian determinant is easy to compute.\nNow let’s check how this formula meets the three aformentioned goals. First is invertability. Simply rearrange the terms and we can get the inverse transformation:\n\\[\n\\begin{align}\n\\mathbf{Z}_A &= \\mathbf{X}_A, \\\\\n\\mathbf{Z}_B &= (\\mathbf{X}_B - b(\\mathbf{X}_A, w)) \\odot \\exp(-s(\\mathbf{X}_A, w)).\n\\end{align}\n\\]\nNotice how the inverse transformation does not involve inverting the neural networks at all, just changing the sign of the \\(\\exp\\) function. This is the key to making the transformation invertible easy and efficient.\nSecond is computing the Jacobian determinant. It turns out the Jacobian is a lower trianglular matrix\n\\[\n\\begin{bmatrix}\n\\mathbf{I} & 0 \\\\\n\\frac{\\partial \\mathbf{Z}_B}{\\partial \\mathbf{X}_A} & \\text{diag}(-\\exp(s(\\mathbf{Z}_A, w)))\n\\end{bmatrix}\n\\]\nand the determinant is simply the product of the diagonal elements.\nTo make the network more expressive, normalizing flows often have multiple coupling layers stacked together, switching the roles of \\(\\mathbf{Z}_A\\) and \\(\\mathbf{Z}_B\\) at each layer, and possibly also changing the split points at each layer. The final data likelihood is the product of the likelihoods at each layer. And the Jacobian determinant is the product of the determinants at each layer.\nThird is sampling. Once the model is trained, we can start with \\(\\mathbf{Z}_A\\), and follow the flow till we get to \\(\\mathbf{X}\\). The sampling process is deterministic and easy to compute.\nAs an example we can train a normalizing flow model on a two-moons dataset, using the normflows package. The following code is adapted from the package’s example code.\n# Import required packages\nimport torch\nimport numpy as np\nimport normflows as nf\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\ndevice = 'cuda'\nThis is the target distribution\n# Define target distribution\ntarget = nf.distributions.TwoMoons()\n# Plot target distribution\ngrid_size = 200\nxx, yy = torch.meshgrid(torch.linspace(-3, 3, grid_size), torch.linspace(-3, 3, grid_size), indexing='xy')\nzz = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\nzz = zz.to(device)\n\nlog_prob = target.log_prob(zz).to('cpu').view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nplt.pcolormesh(xx, yy, prob.data.numpy(), cmap='hot')\nplt.gca().set_aspect('equal', 'box')\nConstruct the model. To define a normalizing flow model, we first specify the base distribution and the transformation layers, and then combine them in the NormalizingFlow class.\n# Define 2D Gaussian base distribution\nbase = nf.distributions.base.DiagGaussian(2)\n\n# Define list of flows\nnum_layers = 2\nflows = []\nfor i in range(num_layers):\n    # Neural network with two hidden layers having 64 units each\n    # Last layer is initialized by zeros making training more stable\n    param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True)\n    # Add flow layer\n    flows.append(nf.flows.AffineCouplingBlock(param_map))\n    # Swap dimensions\n    flows.append(nf.flows.Permute(2, mode='swap'))\n\nmodel = nf.NormalizingFlow(base, flows)\nmodel = model.to(device)\nTrain the model.\nmax_iter = 3201\nnum_samples = 2 ** 9\nshow_iter = 800\n\nloss_hist = np.array([])\nprob_list = []\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n\nfor it in tqdm(range(max_iter)):\n    optimizer.zero_grad()\n\n    # Get training samples\n    x = target.sample(num_samples).to(device)\n\n    # Compute loss\n    loss = model.forward_kld(x)\n\n    # Do backprop and optimizer step\n    if ~(torch.isnan(loss) | torch.isinf(loss)):\n        loss.backward()\n        optimizer.step()\n\n    # Log loss\n    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n\n    # Save prob for later plotting\n    if it % show_iter == 0:\n        model.eval()\n        log_prob = model.log_prob(zz)\n        model.train()\n        prob = torch.exp(log_prob.to('cpu').view(*xx.shape))\n        prob[torch.isnan(prob)] = 0\n        prob_list.append(prob.data.numpy())\nplt.plot(loss_hist)\n\n  0%|          | 0/3201 [00:00&lt;?, ?it/s]  0%|          | 15/3201 [00:00&lt;00:21, 149.57it/s]  2%|▏         | 64/3201 [00:00&lt;00:09, 348.03it/s]  3%|▎         | 111/3201 [00:00&lt;00:07, 400.08it/s]  5%|▍         | 157/3201 [00:00&lt;00:07, 420.78it/s]  6%|▋         | 202/3201 [00:00&lt;00:06, 430.97it/s]  8%|▊         | 248/3201 [00:00&lt;00:06, 438.31it/s]  9%|▉         | 294/3201 [00:00&lt;00:06, 442.53it/s] 11%|█         | 339/3201 [00:00&lt;00:06, 444.55it/s] 12%|█▏        | 384/3201 [00:00&lt;00:06, 446.14it/s] 13%|█▎        | 430/3201 [00:01&lt;00:06, 448.35it/s] 15%|█▍        | 475/3201 [00:01&lt;00:06, 448.17it/s] 16%|█▌        | 520/3201 [00:01&lt;00:05, 447.81it/s] 18%|█▊        | 566/3201 [00:01&lt;00:05, 448.81it/s] 19%|█▉        | 611/3201 [00:01&lt;00:05, 445.47it/s] 20%|██        | 656/3201 [00:01&lt;00:05, 446.51it/s] 22%|██▏       | 701/3201 [00:01&lt;00:05, 446.64it/s] 23%|██▎       | 746/3201 [00:01&lt;00:05, 446.91it/s] 25%|██▍       | 791/3201 [00:01&lt;00:05, 447.78it/s] 26%|██▌       | 836/3201 [00:01&lt;00:05, 438.08it/s] 28%|██▊       | 881/3201 [00:02&lt;00:05, 441.20it/s] 29%|██▉       | 926/3201 [00:02&lt;00:05, 441.27it/s] 30%|███       | 971/3201 [00:02&lt;00:05, 442.14it/s] 32%|███▏      | 1017/3201 [00:02&lt;00:04, 444.51it/s] 33%|███▎      | 1062/3201 [00:02&lt;00:04, 444.95it/s] 35%|███▍      | 1107/3201 [00:02&lt;00:04, 446.39it/s] 36%|███▌      | 1152/3201 [00:02&lt;00:04, 446.95it/s] 37%|███▋      | 1197/3201 [00:02&lt;00:04, 443.93it/s] 39%|███▉      | 1242/3201 [00:02&lt;00:04, 445.48it/s] 40%|████      | 1287/3201 [00:02&lt;00:04, 445.54it/s] 42%|████▏     | 1332/3201 [00:03&lt;00:04, 446.29it/s] 43%|████▎     | 1377/3201 [00:03&lt;00:04, 445.23it/s] 44%|████▍     | 1422/3201 [00:03&lt;00:04, 444.74it/s] 46%|████▌     | 1467/3201 [00:03&lt;00:03, 446.30it/s] 47%|████▋     | 1512/3201 [00:03&lt;00:03, 446.52it/s] 49%|████▊     | 1557/3201 [00:03&lt;00:03, 446.73it/s] 50%|█████     | 1602/3201 [00:03&lt;00:03, 444.11it/s] 51%|█████▏    | 1647/3201 [00:03&lt;00:03, 441.48it/s] 53%|█████▎    | 1692/3201 [00:03&lt;00:03, 443.78it/s] 54%|█████▍    | 1737/3201 [00:03&lt;00:03, 445.33it/s] 56%|█████▌    | 1782/3201 [00:04&lt;00:03, 445.83it/s] 57%|█████▋    | 1828/3201 [00:04&lt;00:03, 447.55it/s] 59%|█████▊    | 1873/3201 [00:04&lt;00:02, 447.12it/s] 60%|█████▉    | 1918/3201 [00:04&lt;00:02, 447.26it/s] 61%|██████▏   | 1963/3201 [00:04&lt;00:02, 447.33it/s] 63%|██████▎   | 2008/3201 [00:04&lt;00:02, 447.37it/s] 64%|██████▍   | 2053/3201 [00:04&lt;00:02, 447.26it/s] 66%|██████▌   | 2098/3201 [00:04&lt;00:02, 447.30it/s] 67%|██████▋   | 2143/3201 [00:04&lt;00:02, 447.96it/s] 68%|██████▊   | 2188/3201 [00:04&lt;00:02, 448.04it/s] 70%|██████▉   | 2234/3201 [00:05&lt;00:02, 448.40it/s] 71%|███████   | 2279/3201 [00:05&lt;00:02, 447.09it/s] 73%|███████▎  | 2324/3201 [00:05&lt;00:01, 446.44it/s] 74%|███████▍  | 2370/3201 [00:05&lt;00:01, 447.81it/s] 75%|███████▌  | 2415/3201 [00:05&lt;00:01, 444.66it/s] 77%|███████▋  | 2461/3201 [00:05&lt;00:01, 447.19it/s] 78%|███████▊  | 2506/3201 [00:05&lt;00:01, 445.27it/s] 80%|███████▉  | 2551/3201 [00:05&lt;00:01, 445.60it/s] 81%|████████  | 2596/3201 [00:05&lt;00:01, 446.04it/s] 83%|████████▎ | 2642/3201 [00:05&lt;00:01, 446.93it/s] 84%|████████▍ | 2687/3201 [00:06&lt;00:01, 444.15it/s] 85%|████████▌ | 2732/3201 [00:06&lt;00:01, 444.66it/s] 87%|████████▋ | 2777/3201 [00:06&lt;00:00, 445.03it/s] 88%|████████▊ | 2822/3201 [00:06&lt;00:00, 445.70it/s] 90%|████████▉ | 2867/3201 [00:06&lt;00:00, 446.12it/s] 91%|█████████ | 2912/3201 [00:06&lt;00:00, 446.45it/s] 92%|█████████▏| 2957/3201 [00:06&lt;00:00, 440.97it/s] 94%|█████████▍| 3002/3201 [00:06&lt;00:00, 442.14it/s] 95%|█████████▌| 3047/3201 [00:06&lt;00:00, 442.77it/s] 97%|█████████▋| 3092/3201 [00:06&lt;00:00, 444.39it/s] 98%|█████████▊| 3137/3201 [00:07&lt;00:00, 445.22it/s] 99%|█████████▉| 3182/3201 [00:07&lt;00:00, 445.20it/s]100%|██████████| 3201/3201 [00:07&lt;00:00, 442.53it/s]\nPlot the results. We can see that the model has (roughly) learned the distribution of the two moons dataset.\nfig, axes = plt.subplots(1, len(prob_list), figsize=(10, 2), sharey=True)\nfor i, prob in enumerate(prob_list):\n    ax = axes[i]\n    c = ax.pcolormesh(xx, yy, prob, cmap='hot')\n    ax.set_aspect('equal', 'box')\n\n# Adjust the colorbar to have more padding\ncbar = fig.colorbar(c, ax=axes, orientation='vertical', fraction=0.02, pad=0.02)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#coupling-flows",
    "href": "18.html#coupling-flows",
    "title": "3  Normalizing Flows",
    "section": "",
    "text": "The transformation function \\(f\\) should be (easily enough) invertible, so that we can compute the latent variable likelihood.\nThe Jacobian determinant of the transformation should be easy to compute, so that we can correct the latent likelihood to get the data likelihood.\nWe should be able to sample from it. Once the above two are met, this is usually straightforward, although the computation cost might vary.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathbf{Z}_A\\) is an identity transformation of \\(\\mathbf{X}_A\\), so \\(\\frac{\\partial \\mathbf{Z}_A}{\\partial \\mathbf{X}_A}\\) is an identity matrix.\n\\(\\frac{\\partial \\mathbf{Z}_A}{\\partial \\mathbf{X}_B}\\) is zero.\n\\(\\mathbf{Z}_B\\) is \\(\\mathbf{X}_B\\) minus a linear transformation of \\(\\mathbf{X}_A\\) (doesn’t involve \\(\\mathbf{Z}_B\\)), then element-wise multiplied by the exponential term (meaning no interaction among \\(\\mathbf{Z}_B\\)), so \\(\\frac{\\partial \\mathbf{Z}_B}{\\partial \\mathbf{X}_B}\\) is a diagonal matrix, and the diagonal values are the corresponding negatives of the exponential term. Up to this point we know the Jacobian matrix itselve is a lower triangular matrix.\n\\(\\frac{\\partial \\mathbf{Z}_B}{\\partial \\mathbf{X}_A}\\) is more complicated, but it doesn’t factor into the Jacobian determinant so can be safely ignored.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#autoregressive-flows",
    "href": "18.html#autoregressive-flows",
    "title": "3  Normalizing Flows",
    "section": "3.2 autoregressive flows",
    "text": "3.2 autoregressive flows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "18.html#continuous-flows",
    "href": "18.html#continuous-flows",
    "title": "3  Normalizing Flows",
    "section": "3.3 continuous flows",
    "text": "3.3 continuous flows\nIn normalizing flow models, for each transformation layer, the input and output always have the same dimensionality, we are thus looking for a more meaningful representation of the same data space. There is another neural network sharing this property, namely residual networks, but there is no guarantee that such a network will be invertible. Here we introduce a well known mathematical concept, the differential equation, into the neural network, and thus satisfying both the invertibility and the constant dimensionality requirements.\n\n3.3.1 neural differential equation\nNeural differential equation, as the name implies, is a neural network that is defined by a differential equation. We can consider the residual network as a discrete version of the differential equation, since the “residual” is already a difference between consecutive layers, and the differential is the limit of this difference as it approaches zero. Thus starting from a residual network\n\\[\n\\mathbf{z}_{t+1} = \\mathbf{z}_t + f(\\mathbf{z}_t, \\mathbf{w})\n\\]\nwe can readily convert it into a differential equation\n\\[\n\\frac{d\\mathbf{z(t)}}{dt} = f(\\mathbf{z(t)}, \\mathbf{w}).\n\\]\nNow defining something is easy, what really matters is what we can do with it. On the modeling side, starting from an initial state \\(\\mathbf{z}_0\\), we no longer need to define the number of layers in the network. we can integrate the differential equation to get the state at any time \\(t\\),\n\n\n\n3.3.2 neural ODE backpropagation\n\n\n3.3.3 neural ODE flows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Normalizing Flows</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "20.html",
    "href": "20.html",
    "title": "3  Diffusion Models",
    "section": "",
    "text": "3.1 forward encoder\nfollowing the Bayesian tradition, we denote the hidden variables, i.e. the noisy images as z, and the observed image as x.\nthe forward encoder transforms an image to pure Gaussian noise, by adding some noise to the image at each step.\n\\[\nq(\\mathbf{z}_1 | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z}_1 | \\sqrt{1 - \\boldsymbol{\\beta} _1} \\mathbf{x}, \\boldsymbol{\\beta}_1 \\mathbf{I}).\n\\]\n\\[\nq(\\mathbf{z}_t | \\mathbf{z}_{t-1}) = \\mathcal{N}(\\mathbf{z}_t | \\sqrt{1 - \\boldsymbol{\\beta}_t} \\, \\mathbf{z}_{t-1}, \\boldsymbol{\\beta}_t \\mathbf{I}).\n\\]\nWe then calculate the conditional (on observed data) distributions of the hidden variables, and derive the reverse conditional (on next step hidden variable) distribution using Bayes’ rule.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diffusion Models</span>"
    ]
  },
  {
    "objectID": "20.html#forward-encoder",
    "href": "20.html#forward-encoder",
    "title": "3  Diffusion Models",
    "section": "",
    "text": "3.1.1 diffusion kernel\nbecause each step of the transformation is Gaussian, we can combine the Gaussian transformations between any two random points in time; for this reason, we can derive the hidden variable distribution at any time using just the original image, and the noise schedule up to that time.\n\\[\nq(\\mathbf{z}_1, \\ldots, \\mathbf{z}_t | \\mathbf{x}) = q(\\mathbf{z}_1 | \\mathbf{x}) \\prod_{\\tau=2}^{t} q(\\mathbf{z}_{\\tau} | \\mathbf{z}_{\\tau-1}). \\quad (20.5)\n\\]\n\\[\nq(\\mathbf{z}_t | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z}_t | \\sqrt{\\alpha_t} \\mathbf{x}, (1 - \\alpha_t) \\mathbf{I}). \\quad (20.6)\n\\]\n\\[\n\\alpha_t = \\prod_{\\tau=1}^{t} (1 - \\beta_{\\tau}). \\quad (20.7)\n\\]\n\\[\n\\mathbf{z}_t = \\sqrt{\\alpha_t} \\mathbf{x} + \\sqrt{1 - \\alpha_t} \\boldsymbol{\\epsilon}_t \\quad (20.8)\n\\]\n\\[\nq(\\mathbf{z}_T | \\mathbf{x}) = \\mathcal{N}(\\mathbf{z}_T | \\mathbf{0}, \\mathbf{I}) \\quad (20.9)\n\\]\n\\[\nq(\\mathbf{z}_T) = \\mathcal{N}(\\mathbf{z}_T | \\mathbf{0}, \\mathbf{I}). \\quad (20.10)\n\\]\nsince the final hidden variable is pure Gaussian noise, it does not depend on any other variable.\n\n\n3.1.2 conditional distribuitons\nafter specifying the diffusion kernel, it is of interest to reverse the process, to arrive at the original image from pure Gaussian noise.\nsince we already have the forward diffusion kernel, to get the reverse distribution we’ll turn to the Bayes’ rule, as we have done many times before, and as we’ll do many times after.\nhowever this turns out to be intractable, because to obtain the reverse conditional distribution, from zt to zt-1, we need the marginal distribution of zt-1, but this is impossible, since we have to integrate over the unknown data distribution.\n\\[\nq(\\mathbf{z}_{t-1} | \\mathbf{z}_t) = \\frac{q(\\mathbf{z}_t | \\mathbf{z}_{t-1})q(\\mathbf{z}_{t-1})}{q(\\mathbf{z}_t)} \\quad (20.11)\n\\]\n\\[\nq(\\mathbf{z}_{t-1}) = \\int q(\\mathbf{z}_{t-1} | \\mathbf{x})p(\\mathbf{x}) \\, d\\mathbf{x} \\quad (20.12)\n\\]\na more meleable problem is to obtain the reverse conditional distribution, while also conditioning on the observed image data. This is doable (if the observed data is available), since this is exactly the diffusion kernel we obtained in section 20.1.1.\n\\[\nq(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{x}) = \\frac{q(\\mathbf{z}_t | \\mathbf{z}_{t-1}, \\mathbf{x})q(\\mathbf{z}_{t-1} | \\mathbf{x})}{q(\\mathbf{z}_t | \\mathbf{x})} \\quad (20.13)\n\\]\n\\[\nq(\\mathbf{z}_t | \\mathbf{z}_{t-1}, \\mathbf{x}) = q(\\mathbf{z}_t | \\mathbf{z}_{t-1}) \\quad (20.14)\n\\]\n\\[\nq(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{x}) = \\mathcal{N}(\\mathbf{z}_{t-1} | \\mathbf{m}_t(\\mathbf{x}, \\mathbf{z}_t), \\sigma_t^2 \\mathbf{I}) \\quad (20.15)\n\\]\nall the distributions involved are Gaussian, so the density function can be solved analytically (by completing the square)\n\\[\n\\mathbf{m}_t(\\mathbf{x}, \\mathbf{z}_t) = \\frac{(1 - \\alpha_{t-1}) \\sqrt{1 - \\beta_t}\\mathbf{z}_t + \\sqrt{ \\alpha_{t-1}} \\beta_t\\mathbf{x}}{1 - \\alpha_t} \\quad (20.16)\n\\]\n\\[\n\\sigma_t^2 = \\frac{\\beta_t(1 - \\alpha_{t-1})}{1 - \\alpha_t} \\quad (20.17)\n\\]\nthis is very neat but when we generate new data from the model we don’t normally already have the image we want. for this reason we learn a neural network to do the generation instead.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diffusion Models</span>"
    ]
  },
  {
    "objectID": "20.html#reverse-decoder",
    "href": "20.html#reverse-decoder",
    "title": "3  Diffusion Models",
    "section": "3.2 reverse decoder",
    "text": "3.2 reverse decoder\nSo the basic idea is to find a distribution p to approximate reverse distribution q. We’ll parameterise p with a deep neural network and train it to optimise the parameters, so that when the original data is available, the conditional distribution can reproduce the original image as faithfully as possible; and when origial image not available, the model has been well trained to generate images from the data distribution.\n\\[\np(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{w}) = \\mathcal{N}(\\mathbf{z}_{t-1} | \\mathbf{\\mu} (\\mathbf{z}_t, \\mathbf{w}, t), \\beta_t \\mathbf{I}) \\quad (20.18)\n\\]\n\\[\np(\\mathbf{x}, \\mathbf{z}_1, \\ldots, \\mathbf{z}_T | \\mathbf{w}) = p(\\mathbf{z}_T) \\prod_{t=2}^{T} p(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{w}) \\, p(\\mathbf{x} | \\mathbf{z}_1, \\mathbf{w}) \\quad (20.19)\n\\]\n \n \nthe reverse conditional distribution, parameterised by a deep neural network\nthe joint distribution of the reverse process, parameterised by the deep neural network\n\n3.2.1 training the decoder\nto train the network, our first attempt is to directly maximise the data likelihood, which can be obtained by marginaling out the hidden variables.\nthis is clearly impossible, since we’d need the posterior in the first place.\n\\[\np(\\mathbf{x}|\\mathbf{w}) = \\int \\cdots \\int p(\\mathbf{x}, \\mathbf{z}_1, \\ldots, \\mathbf{z}_T|\\mathbf{w}) \\, d\\mathbf{z}_1 \\cdots d\\mathbf{z}_T \\quad (20.20)\n\\]\n\n\n3.2.2 evidence lower bound\nas a second attempt, we’ll decompose the data likelihood function into two terms, the ELBO term and the KL divergence term.\nThis can be done, again, by applying the good old rules of probability. Here we decompose the joint distribution of {{observed and hidden}} variables.\n\\[\np(\\mathbf{x},\\mathbf{z}|\\mathbf{w}) = p(\\mathbf{x}|\\mathbf{z},\\mathbf{w})p(\\mathbf{z}|\\mathbf{w}) \\quad (20.24)\n\\]\n\\[\np(\\mathbf{x}|\\mathbf{w}) = \\frac{p(\\mathbf{x},\\mathbf{z}|\\mathbf{w})}{p(\\mathbf{z}|\\mathbf{x},\\mathbf{w})}\n\\]\ntaking the log, and integrate over the hidden variables give us\n\\[\n\\begin{aligned}\n\\ln p(\\mathbf{x}|\\mathbf{w}) &= \\int q(\\mathbf{z}) \\ln p(\\mathbf{x}|\\mathbf{w}) \\, d\\mathbf{z} \\\\\n&= \\int q(\\mathbf{z}) \\ln \\frac{p(\\mathbf{x},\\mathbf{z}|\\mathbf{w})}{p(\\mathbf{z}|\\mathbf{x},\\mathbf{w})} \\, d\\mathbf{z} \\\\\n&= \\int q(\\mathbf{z}) \\ln \\frac{p(\\mathbf{x},\\mathbf{z}|\\mathbf{w}) q(\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x},\\mathbf{w}) q(\\mathbf{z})} \\, d\\mathbf{z} \\\\\n&= \\int q(\\mathbf{z}) \\ln \\frac{p(\\mathbf{x},\\mathbf{z}|\\mathbf{w})}{q(\\mathbf{z})} \\, d\\mathbf{z} - \\int q(\\mathbf{z}) \\ln \\frac{p(\\mathbf{z}|\\mathbf{x},\\mathbf{w})}{q(\\mathbf{z})} \\, d\\mathbf{z} \\\\\n&= \\mathcal{L}(\\mathbf{w}) + \\text{KL}(q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x},\\mathbf{w})) \\quad (20.21)\n\\end{aligned}\n\\]\nthe KL divergence term is the difference between two distributions, the model of hidden variables, and an approximate posterior of choice.\n\\[\n\\text{KL}(q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x},\\mathbf{w})) = -\\int q(\\mathbf{z}) \\ln \\frac{p(\\mathbf{z}|\\mathbf{x},\\mathbf{w})}{q(\\mathbf{z})} \\, d\\mathbf{z} \\quad (20.23)\n\\]\nthe ELBO term is another integral term, integrating over the same approximate posterior. We can use the ELBO as a surrogate target, because the KL divergence term is non-negative, as such maximising ELBO also maximises the data likelihood.\n\\[\n\\mathcal{L}(\\mathbf{w}) = \\int q(\\mathbf{z}) \\ln \\frac{p(\\mathbf{x}, \\mathbf{z}|\\mathbf{w})}{q(\\mathbf{z})} \\, d\\mathbf{z} \\quad (20.22)\n\\]\nby lower bound we mean\n\\[\n\\ln p(\\mathbf{x}|\\mathbf{w}) \\geq \\mathcal{L}(\\mathbf{w}) \\quad (20.25)\n\\]\nNow let’s take write out the ELBO and take a close look at its components.\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w}) &= \\mathbb{E}_{q} \\left[ \\ln \\frac{p(\\mathbf{z}_T) \\prod_{t=2}^{T}p(\\mathbf{z}_{t-1}|\\mathbf{z}_t, \\mathbf{w}) p(\\mathbf{x} | \\mathbf{z}_1, \\mathbf{w}) }{q(\\mathbf{z}_1|\\mathbf{x}) \\prod_{t=2}^{T}q(\\mathbf{z}_t|\\mathbf{z}_{t-1} , \\mathbf{x})} \\right] \\\\\n&= \\mathbb{E}_{q} \\left[ \\ln p(\\mathbf{z}_T) + \\sum_{t=2}^{T} \\ln \\frac{p(\\mathbf{z}_{t-1}|\\mathbf{z}_t, \\mathbf{w})}{q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1}, \\mathbf{x})} + \\ln p(\\mathbf{z}_1|\\mathbf{w}) + \\ln p(\\mathbf{x}|\\mathbf{z}_1, \\mathbf{w}) \\right] \\quad (20.26)\n\\end{aligned}\n\\]\nwhere \\[\n\\mathbb{E}_{q}[\\cdot] = \\int \\cdots \\int q(\\mathbf{z}_1|\\mathbf{x}) \\left[ \\prod_{t=2}^{T}q(\\mathbf{z}_t|\\mathbf{z}_{t-1}) \\right] [\\cdot] \\, d\\mathbf{z}_1 \\cdots d\\mathbf{z}_T \\quad (20.27)\n\\]\nNote that we can choose ANY approximate posterior function, but the one we ACTUALLY choose is the forward encoder distribution. This is no coincidence, we introduced the forward encoder, exactly for the purpose that it can be used here.\nThe first term of the ELBO is constant w.r.t. the model parameters and is thus ignored.\nFor the second term, we can first sample zt-1 using 20.3, then sample zt using 20.4, then we can compute p and q with no problem. This is procedurally right, however the estimation will be very noisy, since we consecutively did two sampling steps.\nThe third term is constant w.r.t. the model parameters and is thus ignored.\nFor the fourth term we can first sample z1 and then calculate the value.\n\\[\n\\mathbb{E}_{q}[\\ln p(\\mathbf{x}|\\mathbf{z}_1, \\mathbf{w})] \\approx \\frac{1}{L} \\sum_{l=1}^{L} \\ln p(\\mathbf{x}|\\mathbf{z}_1^{(l)}, \\mathbf{w}) \\quad (20.28)\n\\]\n\n\n3.2.3 rewriting the ELBO\nAs a third attempt, we will further decompose the second term of the ELBO, again by applying the good old rules of probability. This time we apply them on the joint distribution of the adjacent latent variables in the forward encoder.\nwe have the reverse process in the numerator, we want to also have the reverse process in the denominator to match it.\n\\[\nq(\\mathbf{z}_t | \\mathbf{z}_{t-1}, \\mathbf{x}) = \\frac{q(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{x})q(\\mathbf{z}_t | \\mathbf{x})}{q(\\mathbf{z}_{t-1} | \\mathbf{x})} \\quad (20.29)\n\\]\n\\[\n\\ln \\frac{p(\\mathbf{z}_{t-1}|\\mathbf{z}_t, \\mathbf{w})}{q(\\mathbf{z}_t|\\mathbf{z}_{t-1}, \\mathbf{x})} = \\ln \\frac{p(\\mathbf{z}_{t-1}|\\mathbf{z}_t, \\mathbf{w})}{q(\\mathbf{z}_{t-1}|\\mathbf{z}_t, \\mathbf{x})} + \\ln \\frac{q(\\mathbf{z}_{t-1}|\\mathbf{x})}{q(\\mathbf{z}_t|\\mathbf{x})} \\quad (20.30)\n\\]\nThe second term on the RHS is independent of model parameters (remember they are defined by the forward encoder) and is thus ignored. The first term is two distributions of the same variable, so we are again dealing with a KL divergence term.\nUsing this, we can rewrite the ELBO as 20.31\nNow the ELBO consist of two terms, which we call the reconstruction term and the consistency term.\nThe reconstruction term is the logp weighted by z1, and thus measures how much we can reconstruct the data once the latent variable distribution is know. The consistency term is the KL divergence between the forward diffusion distribution and the reverse decoder network. So now we are finally able to connect what we need to compute, the (approximate) data likelihood, to what we have actually defined, the forward diffusion process and the backward denoising neural network. And we can happily proceed with the parameter optimization that we are so fond of.\nSince the two distributions in the KL divergence are both Gaussian, the divergence can be easily computed. 20.33\n\n\n3.2.4 predicting the noise\nwe are already able to optimize the model parameters, by matching the reverse denoising network to the forward diffusion network, by representing the KL devergence as the squared difference between the means of the forward and reverse process.\nit turns out we can use a simply trick, not modeling the mean of the reverse process, but the noise added to the latent variable, to improve the training and the generated image quality.\nit also turns out that when using this formulation, the reconstruction term can be viewd as a special case of the consistency term and merged together. So now we are left with a simple training procedure as follows:\n\n\n\n3.2.5 generating new samples",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diffusion Models</span>"
    ]
  },
  {
    "objectID": "20.html#score-matching",
    "href": "20.html#score-matching",
    "title": "3  Diffusion Models",
    "section": "3.3 score matching",
    "text": "3.3 score matching\nThe denoising diffusion models discussed so far in this chapter are closely related to another class of deep generative models that were developed relatively independently and which are based on score matching. These make use of the score function or Stein score, which is defined as the gradient of the log likelihood with respect to the data vector\nHere it is important to emphasize that the gradient is with respect to the data vector, not with respect to any parameter vector. Since the score function is the gradient wrt the input data\n\nit is vector-valued, and has the same dimensionality as the input.\nintegrating the score function over the data space gives the log data likelihood, up to a constant.\n\nSo if we parameterize the score function as s(x, w), optimize the parameters w so that the parameterized score matches the empirical data score, we have effectively modeled the data distribution. And if we can further generate samples using the parameterized score function, we have built a generative model of the data that we can sample from.\nFigure 20.5 shows an example of a probability density in two dimensions, along with the corresponding score function.\ndefine a model for the data, calculate the logp and the score function define a second model for the score, optimizing the parameters by matching the data score function do sampling using the parameterized score model\n \n\n3.3.1 score loss function\nthe training match the model score to the data distribution score.\n\\[\nJ(\\mathbf{w}) = \\frac{1}{2} \\int \\left\\| \\mathbf{s}(\\mathbf{x}, \\mathbf{w}) - \\nabla_{\\mathbf{x}} \\ln p(\\mathbf{x}) \\right\\|^2 p(\\mathbf{x}) \\, d\\mathbf{x} \\quad (20.43)\n\\]\nof course this is only theoretically; we don’t actually know the data distribution. we now move on to workarounds.\n\n\n3.3.2 modified score loss\nwe first approximate the data distribution with the empirical one \\[\np_{\\mathcal{D}}(\\mathbf{x}) = \\frac{1}{N} \\sum_{n=1}^{N} \\delta(\\mathbf{x} - \\mathbf{x}_n) \\quad (20.44)\n\\]\nsince the empirical distribution is not differentiable, we then smooth it with the Parzen estimator \\[\nq_{\\sigma}(\\mathbf{z}) = \\int q(\\mathbf{z} | \\mathbf{x}, \\sigma)p(\\mathbf{x}) \\, d\\mathbf{x} \\quad (20.47)\n\\]\nusing this result the loss function can be modified to \\[\nJ(\\mathbf{w}) = \\frac{1}{2N} \\sum_{n=1}^{N} \\int \\left\\| \\mathbf{s}(\\mathbf{z}, \\mathbf{w}) - \\nabla_{\\mathbf{z}} \\ln q(\\mathbf{z}|\\mathbf{x}_n, \\sigma) \\right\\|^2 q(\\mathbf{z}|\\mathbf{x}_n, \\sigma) \\, d\\mathbf{z} + \\text{const.} \\quad (20.51)\n\\]\nin the end, note we are matching scores of the latents, not the data.\n\n\n3.3.3 noise variance\n\n\n3.3.4 stochastic differential equations\nWe have seen that it is helpful to use a large number of steps, often several thousand, when constructing the noise process for a diffusion model. It is therefore natural to ask what happens if we consider the limit of an infinite number of steps, much as we did for infinitely deep neural networks when we introduced neural differential equations. In taking such a limit, we need to ensure that the noise variance \\(\\hat{\\beta}_t\\) at each step becomes smaller in keeping with the step size. This leads to a formulation of diffusion models for continuous time as stochastic differential equations. Both denoising diffusion probabilistic models and score matching models can then be viewed as a discretization of a continuous-time SDE.\nthe forward process We can write a general SDE as an infinitesimal update to the vector \\(\\mathbf{z}\\) in the form\n\\[\ndz = f(\\mathbf{z}, t) \\, dt + g(t) \\, d\\mathbf{v} \\quad (20.55)\n\\]\nwith drift and diffusion terms, respectively, where the drift term is deterministic, as in an ODE, but the diffusion term is stochastic, for example given by infinitesimal Gaussian steps. Here the parameter \\(t\\) is often called ‘time’ by analogy with physical systems. The forward noise process (20.3) for a diffusion model can be written as an SDE of the form (20.55) by taking the continuous-time limit.\nthe reverse process rooted in the time-reversibility of stochastic processes\nFor the SDE (20.55), there is a corresponding reverse SDE (Song et al., 2020) given by\n\\[\ndz = \\left\\{ f(\\mathbf{z}, t) - g^2(t) \\nabla_{\\mathbf{z}} \\ln p(\\mathbf{z}) \\right\\} dt + g(t) \\, d\\mathbf{v} \\quad (20.56)\n\\]\nwhere we recognize \\(\\nabla_{\\mathbf{z}} \\ln p(\\mathbf{z})\\) as the score function. The SDE given by (20.55) is to be solved in reverse from \\(t = T\\) to \\(t = 0\\).\nequal-spaced solver To solve an SDE numerically, we need to discretize the time variable. The simplest approach is to use fixed, equally spaced time steps, which is known as the Euler–Maruyama solver. For the reverse SDE, we then recover a form of the Langevin equation. However, more sophisticated solvers can be employed that use more flexible forms of discretization.\nadaptive-step solver For all diffusion processes governed by an SDE, there exists a corresponding deterministic process described by an ODE whose trajectories have the same marginal probability densities \\(p(\\mathbf{z}|t)\\) as the SDE (Song et al., 2020). For an SDE of the form (20.56), the corresponding ODE is given by\n\\[\n\\frac{d\\mathbf{z}}{dt} = f(\\mathbf{z}, t) - \\frac{1}{2} g^2(t) \\nabla_{\\mathbf{z}} \\ln p(\\mathbf{z}). \\quad (20.57)\n\\]\nThe ODE formulation allows the use of efficient adaptive-step solvers to reduce the number of function evaluations dramatically. Moreover, it allows probabilistic diffusion models to be related to normalizing flow models, from which the change-of-variables formula (18.1) can be used to provide an exact evaluation of the log likelihood.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diffusion Models</span>"
    ]
  },
  {
    "objectID": "20.html#guided-diffusion",
    "href": "20.html#guided-diffusion",
    "title": "3  Diffusion Models",
    "section": "3.4 Guided diffusion",
    "text": "3.4 Guided diffusion\n  \n\n3.4.1 classifier guidance\n\n\n3.4.2 classifier-free guidance",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Diffusion Models</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "2  Gradient Descent",
    "section": "",
    "text": "2.1 error surfaces\nerror function, local minimum A, global minimum B, and gradient at C",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "07.html#error-surfaces",
    "href": "07.html#error-surfaces",
    "title": "2  Gradient Descent",
    "section": "",
    "text": "2.1.1 local quadratic approximation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "07.html#gradient-descent-optimisation",
    "href": "07.html#gradient-descent-optimisation",
    "title": "2  Gradient Descent",
    "section": "2.2 gradient descent optimisation",
    "text": "2.2 gradient descent optimisation\nsimply put, gradient descent initialize the weights somewhere, calculate an increment using gradient information, and modify the weights using this information.\n\\[\n\\mathbf{w}^{(\\tau)} = \\mathbf{w}^{(\\tau-1)} + \\Delta \\mathbf{w}^{(\\tau-1)}\n\\]\nthe bulk of our attention will be devoted to how to get the gradient, but the starting point is also quite important.\n\n2.2.1 use of gradient information\n\n\n2.2.2 batch gradient descent\n\n\n2.2.3 stochastic gradient descent\n\n\n2.2.4 mini-batches\n\n\n2.2.5 parameter initialisation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "07.html#convergence",
    "href": "07.html#convergence",
    "title": "2  Gradient Descent",
    "section": "2.3 convergence",
    "text": "2.3 convergence\n\n\n2.3.1 momentum\n  \n\n\n2.3.2 learning rate schedule\n\n\n2.3.3 RMSProp and Adam",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "07.html#normalisation",
    "href": "07.html#normalisation",
    "title": "2  Gradient Descent",
    "section": "2.4 normalisation",
    "text": "2.4 normalisation\nThe gopher(2022) paper also introduced a RMSNorm normalisation method.\n\n2.4.1 data normalisation\n\n\n\n2.4.2 batch normalisation\n\n\n\n2.4.3 layer normalisation\n\n\nimport torch\nimport torch.nn as nn\nfrom einops import reduce\n\n# NLP Example\nbatch, sentence_length, embedding_dim = 2, 5, 3\nembedding = torch.randn(batch, sentence_length, embedding_dim)\nlayer_norm = nn.LayerNorm(embedding_dim)\n\n# Calculate mean before LayerNorm across all dimensions except batch\nmean_before = reduce(embedding, 'b s e -&gt; b', 'mean')\n\n# Calculate variance before LayerNorm across all dimensions except batch\nvar_before = reduce((embedding - mean_before.reshape(batch, 1, 1)) ** 2, 'b s e -&gt; b', 'mean')\n\n# Activate module\nnormalized_embedding = layer_norm(embedding)\n\n# Calculate mean after LayerNorm across all dimensions except batch\nmean_after = reduce(normalized_embedding, 'b s e -&gt; b', 'mean')\n\n# Calculate variance after LayerNorm across all dimensions except batch\nvar_after = reduce((normalized_embedding - mean_after.reshape(batch, 1, 1)) ** 2, 'b s e -&gt; b', 'mean')\n\n# Display mean and variance\nprint(\"Before LayerNorm - Mean: \", end=\"\")\nprint(*mean_before.tolist(), sep=\", \")\nprint(\"Before LayerNorm - Variance: \", end=\"\")\nprint(*var_before.tolist(), sep=\", \")\n\nprint(\"After LayerNorm - Mean: \", end=\"\")\nprint(*mean_after.tolist(), sep=\", \")\nprint(\"After LayerNorm - Variance: \", end=\"\")\nprint(*var_after.tolist(), sep=\", \")\n\nBefore LayerNorm - Mean: -0.1163964793086052, 0.16730044782161713\nBefore LayerNorm - Variance: 1.2326244115829468, 1.6592100858688354\nAfter LayerNorm - Mean: -7.947286384535346e-09, -7.947286384535346e-09\nAfter LayerNorm - Variance: 0.9999745488166809, 0.9999361634254456",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Probabilities",
    "section": "",
    "text": "2.1 The rules of probability\nuncertainty: collect more data; use a more capable model\nprobability as frequency:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "02.html#the-rules-of-probability",
    "href": "02.html#the-rules-of-probability",
    "title": "2  Probabilities",
    "section": "",
    "text": "2.1.1 A medical screening example\n\n\n\n2.1.2 The sum and product rules\n\n\n\n2.1.3 Bayes’ theorem\n   \n\n\n2.1.4 Medical screening revisited\n\n\n2.1.5 Prior and posterior probabilities\n\n\n2.1.6 Independent variables",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "02.html#probability-densities",
    "href": "02.html#probability-densities",
    "title": "2  Probabilities",
    "section": "2.2 Probability densities",
    "text": "2.2 Probability densities\n\n\n2.2.1 Example distributions\n\n\n\n2.2.2 Expectations and covariances",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "02.html#the-gaussian-distribution",
    "href": "02.html#the-gaussian-distribution",
    "title": "2  Probabilities",
    "section": "2.3 The Gaussian distribution",
    "text": "2.3 The Gaussian distribution\n\\[\n\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp \\left\\{ -\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right\\}\n\\]\n\n2.3.1 Mean and variance\n\n\n2.3.2 Likelihood function\n\n\n2.3.3 Bias of maximum likelihood\n\n\n2.3.4 Linear regression",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "02.html#transformation-of-densities",
    "href": "02.html#transformation-of-densities",
    "title": "2  Probabilities",
    "section": "2.4 Transformation of densities",
    "text": "2.4 Transformation of densities\n\n\n2.4.1 Multivariate distributions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "02.html#information-theory",
    "href": "02.html#information-theory",
    "title": "2  Probabilities",
    "section": "2.5 Information theory",
    "text": "2.5 Information theory\n \nthe broader distribution has a higher entropy. A uniform distribution would have the largest entropy.\n\n2.5.1 Entropy\n\n\n2.5.2 Physics perspective\n\n\n2.5.3 Differential entropy\n\n\n2.5.4 Maximum entropy\n\n\n2.5.5 Kullback–Leibler divergence\n\n\n2.5.6 Conditional entropy\n\n\n2.5.7 Mutual information",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "02.html#bayesian-probabilities",
    "href": "02.html#bayesian-probabilities",
    "title": "2  Probabilities",
    "section": "2.6 Bayesian probabilities",
    "text": "2.6 Bayesian probabilities\nmore general, probability as measure of uncertainty\n\n2.6.1 Model parameters\nusing probability to model parameter uncertainty, and update the probability distribution with data.\n\n\n2.6.2 Regularization\nusing the rules of probability, we can establish a connection between MAP and ML estimation, and see that MAP is in fact a regularised ML estimation.\n\n\n2.6.3 Bayesian machine learning\nfrom parameter uncertainty to outcome uncertainty",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Standard distributions",
    "section": "",
    "text": "3.1 Discrete variables",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard distributions</span>"
    ]
  },
  {
    "objectID": "03.html#discrete-variables",
    "href": "03.html#discrete-variables",
    "title": "3  Standard distributions",
    "section": "",
    "text": "3.1.1 Bernoulli distribution\n\n\n3.1.2 Binomial distribution\n\n\n3.1.3 Multinomial distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard distributions</span>"
    ]
  },
  {
    "objectID": "03.html#the-multivariate-gaussian",
    "href": "03.html#the-multivariate-gaussian",
    "title": "3  Standard distributions",
    "section": "3.2 The multivariate Gaussian",
    "text": "3.2 The multivariate Gaussian\n\n3.2.1 Geometry of the Gaussian\n\n\n3.2.2 Moments\n\n\n3.2.3 Limitations\n\n\n3.2.4 Conditional distribution\n\n\n3.2.5 Marginal distribution\n\n\n3.2.6 Bayes’ theorem\n\n\n3.2.7 Maximum likelihood\n\n\n3.2.8 Sequential estimation\n\n\n3.2.9 mixture of Gaussians",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard distributions</span>"
    ]
  },
  {
    "objectID": "03.html#periodic-variables",
    "href": "03.html#periodic-variables",
    "title": "3  Standard distributions",
    "section": "3.3 Periodic variables",
    "text": "3.3 Periodic variables\n\n3.3.1 von Mises distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard distributions</span>"
    ]
  },
  {
    "objectID": "03.html#the-exponential-family",
    "href": "03.html#the-exponential-family",
    "title": "3  Standard distributions",
    "section": "3.4 The exponential family",
    "text": "3.4 The exponential family\n\\[\np(\\mathbf{x}|\\mathbf{\\eta}) = h(\\mathbf{x})g(\\mathbf{\\eta}) \\exp\\{\\mathbf{\\eta}^T \\mathbf{u}(\\mathbf{x})\\}\n\\]\nThe four key elements of the exponential family are:\n\nbase measure\npartition function\nnatural parameter\nsufficient statistics\n\n\n3.4.1 Sufficient statistics\nDue to the formula constraint of the exponential family, any information about the data that is not in the sufficient statistics is lost.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard distributions</span>"
    ]
  },
  {
    "objectID": "03.html#nonparametric-methods",
    "href": "03.html#nonparametric-methods",
    "title": "3  Standard distributions",
    "section": "3.5 Nonparametric methods",
    "text": "3.5 Nonparametric methods\n\n3.5.1 Histograms\n\n\n3.5.2 Kernel density\n\n\n3.5.3 Nearest neighbors",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Standard distributions</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Single layer networks: regression",
    "section": "",
    "text": "4.1 Linear regression",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single layer networks: regression</span>"
    ]
  },
  {
    "objectID": "04.html#linear-regression",
    "href": "04.html#linear-regression",
    "title": "4  Single layer networks: regression",
    "section": "",
    "text": "4.1.1 Basis function\nExtending simple linear regression with basis functions.\n\nSome basis functions are more localised than others.\n  \n\n\n4.1.2 Likelihood function\n\\[\n\\begin{align}\n\\ln p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\sigma^2) &= \\sum_{n=1}^{N} \\ln \\mathcal{N}(t_n | \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x}_n), \\sigma^2) \\\\\n&= \\sum_{n=1}^{N} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ -\\frac{1}{2\\sigma^2} (t_n - \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x}_n))^2 \\right\\} \\right) \\\\\n&= -\\frac{N}{2} \\ln (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} E_D(\\mathbf{w})\n\\end{align}\n\\]\nwhere the sum-of-squares error function is defined by\n\\[\nE_D(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} \\left\\{ t_n - \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x}_n) \\right\\}^2.\n\\]\n\n\n4.1.3 Maximum likelihood\n\n\n4.1.4 Geometry of least squares\nGeometrically, the least squares solution is the orthogonal projection of the target onto the subspace spanned by the basis functions.\n\n\n\n4.1.5 Sequential learning\n\n\n4.1.6 Regularised least squares\n\n\n4.1.7 Multiple outputs",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single layer networks: regression</span>"
    ]
  },
  {
    "objectID": "04.html#decision-theory",
    "href": "04.html#decision-theory",
    "title": "4  Single layer networks: regression",
    "section": "4.2 Decision theory",
    "text": "4.2 Decision theory",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single layer networks: regression</span>"
    ]
  },
  {
    "objectID": "04.html#the-bias-variance-trade-off",
    "href": "04.html#the-bias-variance-trade-off",
    "title": "4  Single layer networks: regression",
    "section": "4.3 The bias-variance trade-off",
    "text": "4.3 The bias-variance trade-off",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single layer networks: regression</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Single layer networks: classification",
    "section": "",
    "text": "5.1 Discriminant functions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single layer networks: classification</span>"
    ]
  },
  {
    "objectID": "05.html#discriminant-functions",
    "href": "05.html#discriminant-functions",
    "title": "5  Single layer networks: classification",
    "section": "",
    "text": "5.1.1 Two classes\n\n\n5.1.2 Multiple classes\n\n\n5.1.3 1-of-K encoding\n\n\n5.1.4 Least squares for classification",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single layer networks: classification</span>"
    ]
  },
  {
    "objectID": "05.html#decision-theory",
    "href": "05.html#decision-theory",
    "title": "5  Single layer networks: classification",
    "section": "5.2 Decision theory",
    "text": "5.2 Decision theory\n\n5.2.1 Misclassification rate\n\n\n5.2.2 Expected loss\n\n\n5.2.3 The reject option\n\n\n5.2.4 Inference and decision\n\n\n5.2.5 Classification accuracy\n\n\n5.2.6 ROC curve",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single layer networks: classification</span>"
    ]
  },
  {
    "objectID": "05.html#generative-probabilistic-classifiers",
    "href": "05.html#generative-probabilistic-classifiers",
    "title": "5  Single layer networks: classification",
    "section": "5.3 Generative probabilistic classifiers",
    "text": "5.3 Generative probabilistic classifiers\n\n5.3.1 Continuous inputs\n\n\n5.3.2 Maximum likelihood solution\n\n\n5.3.3 Discrete features\n\n\n5.3.4 Exponential family",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single layer networks: classification</span>"
    ]
  },
  {
    "objectID": "05.html#discriminative-probabilistic-classifiers",
    "href": "05.html#discriminative-probabilistic-classifiers",
    "title": "5  Single layer networks: classification",
    "section": "5.4 Discriminative probabilistic classifiers",
    "text": "5.4 Discriminative probabilistic classifiers",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single layer networks: classification</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Deep Neural Networks",
    "section": "",
    "text": "6.1 Limitations of fixed basis functions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deep Neural Networks</span>"
    ]
  },
  {
    "objectID": "06.html#limitations-of-fixed-basis-functions",
    "href": "06.html#limitations-of-fixed-basis-functions",
    "title": "6  Deep Neural Networks",
    "section": "",
    "text": "6.1.1 The curse of dimensionality\n\n\n6.1.2 High dimensional spaces\nshells\n\n\n6.1.3 Data manifolds\ndigits\n\n\n6.1.4 Data-dependent basis functions\nkernel methods",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deep Neural Networks</span>"
    ]
  },
  {
    "objectID": "06.html#multilayer-networks",
    "href": "06.html#multilayer-networks",
    "title": "6  Deep Neural Networks",
    "section": "6.2 Multilayer networks",
    "text": "6.2 Multilayer networks\n\n6.2.1 Parameter matrices\n\n\n6.2.2 Universal approximation\nwith examples\n\n\n6.2.3 Hidden unit activation functions\nsoftmax\n\n\n6.2.4 weight-space symmetries",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deep Neural Networks</span>"
    ]
  },
  {
    "objectID": "06.html#deep-networks",
    "href": "06.html#deep-networks",
    "title": "6  Deep Neural Networks",
    "section": "6.3 Deep networks",
    "text": "6.3 Deep networks\n\n6.3.1 Hierarchical representations\n\n\n6.3.2 Distributed representations\n\n\n6.3.3 Representation learning\n\n\n6.3.4 Transfer learning\n\n\n6.3.5 Contrastive learning\n\n\n6.3.6 General network architectures\n\n\n6.3.7 Tensors",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deep Neural Networks</span>"
    ]
  },
  {
    "objectID": "06.html#error-functions",
    "href": "06.html#error-functions",
    "title": "6  Deep Neural Networks",
    "section": "6.4 Error functions",
    "text": "6.4 Error functions\n\n6.4.1 Regression\n\n\n6.4.2 Binary classification\n\n\n6.4.3 Multiclass classification",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deep Neural Networks</span>"
    ]
  },
  {
    "objectID": "06.html#mixture-density-networks",
    "href": "06.html#mixture-density-networks",
    "title": "6  Deep Neural Networks",
    "section": "6.5 Mixture density networks",
    "text": "6.5 Mixture density networks\n\n6.5.1 Robot kinematics example\n\n\n6.5.2 Conditional mixture distributions\n\n\n6.5.3 Gradient optimization\n\n\n6.5.4 Predictive distribution",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deep Neural Networks</span>"
    ]
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Backpropagation",
    "section": "",
    "text": "8.1 Evaluation of gradient",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Backpropagation</span>"
    ]
  },
  {
    "objectID": "08.html#evaluation-of-gradient",
    "href": "08.html#evaluation-of-gradient",
    "title": "8  Backpropagation",
    "section": "",
    "text": "8.1.1 Single layer networks\n\n\n8.1.2 General feed-forward networks\n\n\n8.1.3 A simple example\n\n\n8.1.4 Numerical differentiation\n\n\n8.1.5 The Jacobian matrix\n\n\n8.1.6 The Hessian matrix",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Backpropagation</span>"
    ]
  },
  {
    "objectID": "08.html#automatic-differentiation",
    "href": "08.html#automatic-differentiation",
    "title": "8  Backpropagation",
    "section": "8.2 Automatic differentiation",
    "text": "8.2 Automatic differentiation\n\n8.2.1 Forward-mode autodiff\n\n\n8.2.2 Reverse-mode autodiff",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Backpropagation</span>"
    ]
  },
  {
    "objectID": "09.html",
    "href": "09.html",
    "title": "9  Regularisation",
    "section": "",
    "text": "9.1 Inductive bias\nInductive bias refers to the set of assumptions a model makes about the patterns it expects to find in the data. These biases help the model to generalize better from the training data to unseen instances, guiding the learning process towards more plausible solutions given limited information. In the context of deep learning, inductive bias can be inherent in the architecture of the network, the choice of optimization algorithm, or the data representation itself.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "09.html#inductive-bias",
    "href": "09.html#inductive-bias",
    "title": "9  Regularisation",
    "section": "",
    "text": "9.1.1 Inverse problem\nIn the Bayesian sense, the forward problem is sampling, the inverse problem is inference. data –&gt; parameter\n\n\n9.1.2 No free lunch theorem\nSince there is no universally best learner for every task, the success of a learning algorithm on a particular task depends on how well its inductive bias aligns with the problem.\nRegularization methods contribute to an algorithm’s inductive bias, thus affecting its performance across different tasks.\nmodels are essentially a set of assumptions, and assumptions have consequences\n\n\n9.1.3 Symmetry and invariance\n\n\n9.1.4 Equivariance",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "09.html#weight-decay",
    "href": "09.html#weight-decay",
    "title": "9  Regularisation",
    "section": "9.2 Weight decay",
    "text": "9.2 Weight decay\nsmaller weights, simpler models L2 regularisation\n\n9.2.1 Consistent regularizers\n\n\n9.2.2 Generalised weight decay",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "09.html#learning-curves",
    "href": "09.html#learning-curves",
    "title": "9  Regularisation",
    "section": "9.3 Learning curves",
    "text": "9.3 Learning curves\nOn the modeling side, there can be several different approaches to increase the generalization performance:\n\nincrease the model size (better representative capability);\nincrease the data size (capturing more pattern and less noise);\nregulariation (through weight-decay or other methods);\nimprove the model architecture (inducing more inductive bias for certain problems);\nmodel averaging (reduce the variance in specific models)\n\nAnother factor that influences the bias-variance trade-off is the learning process itself. During optimization of the error function through gradient descent, the training error typically decreases as the model parameters are updated, whereas the error for hold-out data may be non-monotonic. This behaviour can be visualized using learning curves, which plot performance measures such as training set and validation set error as a function of iteration number during an iterative learning process such as stochastic gradient descent. These curves provide insight into the progress of training and also offer a practical methodology for controlling the effective model complexity.\n\n9.3.1 Early stopping\n\n\n9.3.2 Double descent\nDouble descent entends the bias-variance trade-off to the big-data/large-model regime where we have so much data that we can fit a model of any size to it.\n\nIn the classical regime, we have limited data, thus we seek a bias-variance balance. In the modern regime, since we have so much data, increasing the model size always leads to better test results.\nThe black dashed line marks the interpolation threshold, where the model’s capacity is just enough to perfectly fit the training data.\n\nstochastic gradient descent not only makes the training more feasible, but also makes the model more robust.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "09.html#parameter-sharing",
    "href": "09.html#parameter-sharing",
    "title": "9  Regularisation",
    "section": "9.4 Parameter sharing",
    "text": "9.4 Parameter sharing\n\n9.4.1 Soft parameter sharing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "09.html#residual-connections",
    "href": "09.html#residual-connections",
    "title": "9  Regularisation",
    "section": "9.5 Residual connections",
    "text": "9.5 Residual connections",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "09.html#model-averaging",
    "href": "09.html#model-averaging",
    "title": "9  Regularisation",
    "section": "9.6 Model averaging",
    "text": "9.6 Model averaging\n\n9.6.1 Dropout\nRandomly “dropping out” a subset of neurons or connections during training encourages the network to develop redundant representations and prevents co-adaptation of features. The inductive bias here is towards models that are robust to the loss of individual pieces of information, reflecting a kind of ensemble learning within a single model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regularisation</span>"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "10  Convolutional networks",
    "section": "",
    "text": "10.1 Computer vision",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convolutional networks</span>"
    ]
  },
  {
    "objectID": "10.html#computer-vision",
    "href": "10.html#computer-vision",
    "title": "10  Convolutional networks",
    "section": "",
    "text": "10.1.1 Image data\n\n\n10.1.2 Convolutional filters\n\n\n10.1.3 Feature detectors\n\n\n10.1.4 Translation equivariance\n\n\n10.1.5 Padding\n\n\n10.1.6 Strided convolutions\n\n\n10.1.7 Multi-dimensional convolutions\n\n\n10.1.8 Pooling\n\n\n10.1.9 Multilayer convolutions\n\n\n10.1.10 Example network architectures\nLeNet ImageNet VGG16 AlexNet",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convolutional networks</span>"
    ]
  },
  {
    "objectID": "10.html#visualising-trained-cnns",
    "href": "10.html#visualising-trained-cnns",
    "title": "10  Convolutional networks",
    "section": "10.2 Visualising trained CNNs",
    "text": "10.2 Visualising trained CNNs\n\n10.2.1 Visual cortex\n\n\n10.2.2 Visualizing trained filters\n\n\n10.2.3 Saliency maps\n\n\n10.2.4 Adversarial attacks\n\n\n10.2.5 Synthetic images",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convolutional networks</span>"
    ]
  },
  {
    "objectID": "10.html#object-detection",
    "href": "10.html#object-detection",
    "title": "10  Convolutional networks",
    "section": "10.3 Object detection",
    "text": "10.3 Object detection",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convolutional networks</span>"
    ]
  },
  {
    "objectID": "10.html#image-segmentation",
    "href": "10.html#image-segmentation",
    "title": "10  Convolutional networks",
    "section": "10.4 Image segmentation",
    "text": "10.4 Image segmentation",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convolutional networks</span>"
    ]
  },
  {
    "objectID": "10.html#style-transfer",
    "href": "10.html#style-transfer",
    "title": "10  Convolutional networks",
    "section": "10.5 Style transfer",
    "text": "10.5 Style transfer",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convolutional networks</span>"
    ]
  },
  {
    "objectID": "11.html",
    "href": "11.html",
    "title": "11  structured distributions",
    "section": "",
    "text": "11.1 Graphical models",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>structured distributions</span>"
    ]
  },
  {
    "objectID": "11.html#conditional-independence",
    "href": "11.html#conditional-independence",
    "title": "11  structured distributions",
    "section": "11.2 Conditional independence",
    "text": "11.2 Conditional independence",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>structured distributions</span>"
    ]
  },
  {
    "objectID": "11.html#sequence-models",
    "href": "11.html#sequence-models",
    "title": "11  structured distributions",
    "section": "11.3 Sequence models",
    "text": "11.3 Sequence models",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>structured distributions</span>"
    ]
  },
  {
    "objectID": "12.html",
    "href": "12.html",
    "title": "12  Transformers",
    "section": "",
    "text": "12.1 Attention\nattention was originally developed as an enhancement to RNNs for machine translation, however Vaswani et al. (2017) later showed that \"attention is all you need\", i.e. significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism. Today, transformers based on attention have completely superseded RNNs in almost all applications.\nWe will motivate the use of attention using natural language as an example, although it has much broader applicability. Consider the word \"bank\" in the following sentence: I swam across the river to get to the other bank.\nthe word ‘bank’ can mean different things, However, this can be detected only by looking at the context provided by other words in the sequence. We also see that some words are more important than others in determining the interpretation of ‘bank’. We see that to determine the appropriate interpretation of ‘bank’, a neural network processing such a sentence should attend to specific words from the rest of the sequence.\nMoreover, we also see that the particular locations that should receive more attention depend on the input sequence itself: in the first sentence it is the second and fifth words that are important whereas in the second sentence it is the eighth word. In a standard neural network, different inputs will influence the output to different extents according to the values of the weights that multiply those inputs. Once the network is trained, however, those weights, and their associated inputs, are fixed. By contrast, attention uses weighting factors whose values depend on the specific input data. Figure 12.2 shows the attention weights from a section of a transformer network trained on natural language.\nFigure 12.2 An example of learned attention weights. [From Vaswani et al. (2017) with permission.] The law will never be perfect, but its application should be just, this is what we are missing, in my opinion.\nIn natural language processing (NLP) we need to embed words into vectors, and these vectors can then be used as inputs for subsequent neural network processing. These embeddings capture elementary semantic properties, for example by mapping words with similar meanings to nearby locations in the embedding space. However such embeddings are one-to-one and deterministic, a given word always maps to the same embedding vector. A transformer can be viewed as a richer form of embedding in which a given vector is mapped to a location that depends on the other vectors in the sequence. Thus, the vector representing ‘bank’ in our example above could map to different places in a new embedding space for the two different sentences. For example, in the first sentence the transformed representation might put ‘bank’ close to ‘water’ in the embedding space, whereas in the second sentence the transformed representation might put it close to ‘money’.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformers</span>"
    ]
  },
  {
    "objectID": "12.html#attention",
    "href": "12.html#attention",
    "title": "12  Transformers",
    "section": "",
    "text": "its exact meaning should be determined from the context, i.e. other words in the same sentence.\ndifferent words have different importance in determining its meaning,\nthe words that we should attend to can be anywhere in the sentence.\n\n\nFigure 12.1 Schematic illustration of attention in which the interpretation of the word ‘bank’ is influenced by the words ‘river’ and ‘swam’, with the thickness of each line being indicative of the strength of its influence\n\n\n\n\n\n12.1.1 Transformer processing\nthe structure of the data matrix, of dimension NxD. It’s kept unchange through the transformer layers.\n\nthe input and output have the same dimension, this, coupled with residual networks, is what makes really deep networks possible.\n\n\n12.1.2 Attention coefficients\n\\[\n\\mathbf{y}_n = \\sum_{m=1}^{N} a_{nm} \\mathbf{x}_m\n\\]\nin the attention layer, each output is a weighted sum of all the input.\nattention can be understood in many different ways, a similarity kernel is most useful.\nin this sense, we can think of it as a combination of NN and GP.\n\n\n12.1.3 Self-attention\n\\[\na_{nm} = \\frac{\\exp(\\mathbf{x}_n^T \\mathbf{x}_m)}{\\sum_{m'=1}^{N} \\exp(\\mathbf{x}_n^T \\mathbf{x}_{m'})}\n\\]\nthe weights are calculated as the exponential of the dot product between pairs of inputs.\n\\[\n\\mathbf{Y} = \\text{Softmax}[\\mathbf{X} \\mathbf{X}^T] \\mathbf{X}\n\\]\nno mask, to fully understand the connections in language.\n\n\n12.1.4 Network parameters\nAs it stands, the transformation from input vectors {xn } to output vectors {yn } is fixed and has no capacity to learn from data because it has no adjustable parameters. Furthermore, each of the feature values within a token vector xn plays an equal role in determining the attention coefficients, whereas we would like the network to have the flexibility to focus more on some features than others when determining token similarity.\nWe therefore transform the three input matrices, each with its our transformation, and consequently trainable parameters. The transformed input matrices are named query, key, and value respectively.\n\\[\n\\begin{aligned}\n\\mathbf{Q} &= \\mathbf{X}\\mathbf{W}^{(q)} \\\\\n\\mathbf{K} &= \\mathbf{X}\\mathbf{W}^{(k)} \\\\\n\\mathbf{V} &= \\mathbf{X}\\mathbf{W}^{(v)}\n\\end{aligned}\n\\]\nand the attention layer is accordingly updated to be \\[\n\\mathbf{Y} = \\text{Softmax}(\\mathbf{Q}\\mathbf{K}^T) \\mathbf{V}\n\\]\ndetermining the attention coefficients Q, K, and output Y\n \n\n\n12.1.5 Scaled self-attention\nthe fact that values do not go through the scaling in worth noting.\n \n\n\n12.1.6 Multi-head attention\nin multi-head attention, the weight matrix dimension of the feedfoward layer increases accordings, however the output is kept the same dimension as the input. This is done by adjusting the input and output dimensions of the feed forward layers\n\n \n\n\n12.1.7 Transformer layers\nresidual connection and layer normalisation are added to the multi-head attention\n\n\n\n\n12.1.8 Computational complexity\n\n\n12.1.9 Positional encoding\nPositional encoding injects information about the relative or absolute position of the tokens in a sequence. This is crucial for transformers to capture the order of the sequence, as they do not inherently possess any notion of order due to their attention mechanism.\nBy adding positional encodings to the input embeddings, transformers can leverage the position information to better understand the structure of the data.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformers</span>"
    ]
  },
  {
    "objectID": "12.html#natural-language",
    "href": "12.html#natural-language",
    "title": "12  Transformers",
    "section": "12.2 Natural language",
    "text": "12.2 Natural language\nNow that we have studied the architecture of the transformer, we will explore how this can be used to process language data consisting of words, sentences, and paragraphs. Although this is the modality that transformers were originally developed to operate on, they have proved to be a very general class of models and have become the state-of-the-art for most input data types. Later in this chapter we will look at their use in other domains.\nMany languages, including English, comprise a series of words separated by white space, along with punctuation symbols, and therefore represent an example of sequential data. For the moment we will focus on the words, and we will return to punctuation later.\nThe first challenge is to convert the words into a numerical representation that is suitable for use as the input to a deep neural network. One simple approach is to define a fixed dictionary of words and then introduce vectors of length equal to the size of the dictionary along with a ‘one hot’ representation for each word, in which the kth word in the dictionary is encoded with a vector having a 1 in position k and 0 in all other positions.\nAn obvious problem with a one-hot representation is that a realistic dictionary might have several hundred thousand entries leading to vectors of very high dimensionality. Also, it does not capture any similarities or relationships that might exist between words. Both issues can be addressed by mapping the words into a lower-dimensional space through a process called word embedding in which each word is represented as a dense vector in a space of typically a few hundred dimensions.\n\n12.2.1 Word embedding\nCBOW (continuous bag of words) and skip-gram approach for word embedding\n \n\n\n12.2.2 Tokenisation\n\nSentencePiece\nKarpathy’s tutorial on YT\n\n\n12.2.3 Bag of words\n\n\n12.2.4 Autoregressive models\n\n\n12.2.5 Recurrent neural networks\n \n\n\n12.2.6 Backpropagation through time",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformers</span>"
    ]
  },
  {
    "objectID": "12.html#transformer-language-models",
    "href": "12.html#transformer-language-models",
    "title": "12  Transformers",
    "section": "12.3 Transformer language models",
    "text": "12.3 Transformer language models\nThe transformer processing layer is a highly flexible component for building powerful neural network models with broad applicability. In this section we explore the application of transformers to natural language. This has given rise to the development of massive neural networks known as large language models (LLMs), which have proven to be exceptionally capable (Zhao et al., 2023).\nTransformers can be applied to many different kinds of language processing task, and can be grouped into three categories according to the form of the input and output data. In a problem such as sentiment analysis, we take a sequence of words as input and provide a single variable representing the sentiment of the text, for example happy or sad, as output. Here a transformer is acting as an encoder of the sequence. Other problems might take a single vector as input and generate a word sequence as output, for example if we wish to generate a text caption given an input image. In such cases the transformer functions as a decoder, generating a sequence as output. Finally, in sequence-to-sequence processing tasks, both the input and the output comprise a sequence of words, for example if our goal is to translate from one language to another. In this case, transformers are used in both encoder and decoder roles. We discuss each of these classes of language model in turn, using illustrative examples of model architectures.\n\n12.3.1 Decoder transformers\n \n\n\n12.3.2 Sampling strategies\n\n\n\n12.3.3 Encoder transformers\n\n\n\n12.3.4 Seq2seq transformers\n \n\n\n12.3.5 Large language models",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformers</span>"
    ]
  },
  {
    "objectID": "12.html#multimodal-transformers",
    "href": "12.html#multimodal-transformers",
    "title": "12  Transformers",
    "section": "12.4 Multimodal transformers",
    "text": "12.4 Multimodal transformers\nAlthough transformers were initially developed as an alternative to recurrent networks for processing sequential language data, they have become prevalent in nearly all areas of deep learning. They have proved to be general-purpose models, as they make very few assumptions about the input data, in contrast, for example, to convolutional networks, which make strong assumptions about equivariances and locality. Due to their generality, transformers have become the state-of-the-art for many different modalities, including text, image, video, point cloud, and audio data, and have been used for both discriminative and generative applications within each of these domains. The core architecture of the transformer layer has remained relatively constant, both over time and across applications. Therefore, the key innovations that enabled the use of transformers in areas other than natural language have largely focused on the representation and encoding of the inputs and outputs.\nOne big advantage of a single architecture that is capable of processing many different kinds of data is that it makes multimodal computation relatively straightforward. In this context, multimodal refers to applications that combine two or more different types of data, either in the inputs or outputs or both. For example, we may wish to generate an image from a text prompt or design a robot that can combine information from multiple sensors such as cameras, radar, and microphones. The important thing to note is that if we can tokenize the inputs and decode the output tokens, then it is likely that we can use a transformer.\nFor the input, as long as we can encode and align the inputs from different modalities, we can simply dump them all to the transformer. However, if the output is multimodal, we need to specify which output nodes correspond to which output.\n\n12.4.1 Vision transformers\nIllustration of the vision transformer architecture for a classification task. Here a learnable hclassi token is included as an additional input, and the associated output is transformed by a linear layer with a softmax activation, denoted by LSM, to give the final class-vector output c.\n\n\n\n12.4.2 Generative image transformers\n\na raster scan that defines a specific linear ordering of the pixels in a two-dimensional image.\n\nAn illustration of how an image can be sampled from an autoregressive model. The first pixel is sampled from the marginal distribution p(x11), the second pixel from the conditional distribution p(x12 |x11 ), and so on in raster scan order until we have a complete image.\n\n\n12.4.3 Audio data\nAn example mel spectrogram of a humpback whale song.\n\n\n\n12.4.4 Text-to-speech\n VALL-E\n\n\n12.4.5 Vision and language transformers",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformers</span>"
    ]
  },
  {
    "objectID": "13.html",
    "href": "13.html",
    "title": "13  Graph neural networks",
    "section": "",
    "text": "13.1 Machine learning on graphs\nGraph properties include features such as connectivity, the presence of cycles, community structure, and other characteristics that can inform both the structure of the neural network model and the features used within those models.\nAn adjacency matrix is a fundamental way to represent a graph. It captures the presence or absence (and sometimes the weight) of edges between nodes in the graph. This matrix plays a critical role in the operations of GNNs, particularly in defining graph convolution operations.\nPermutation Equivariance is a desirable property in graph neural networks, ensuring that the output of the GNN does not change when the nodes of the input graph are reordered. This property is crucial for learning consistent representations of graphs regardless of node ordering.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Graph neural networks</span>"
    ]
  },
  {
    "objectID": "13.html#machine-learning-on-graphs",
    "href": "13.html#machine-learning-on-graphs",
    "title": "13  Graph neural networks",
    "section": "",
    "text": "13.1.1 Graph properties\n\n\n13.1.2 Adjacency matrix\n  \n\n\n13.1.3 Permutation equivariance",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Graph neural networks</span>"
    ]
  },
  {
    "objectID": "13.html#neural-message-passing",
    "href": "13.html#neural-message-passing",
    "title": "13  Graph neural networks",
    "section": "13.2 Neural message passing",
    "text": "13.2 Neural message passing\n\n13.2.1 Convolutional filters\n \n\n\n13.2.2 Graph convolution networks (GCN)\nEach node has an associated hidden variable that stores the aggragated info from all its neighbours. Each layer of the network updates the node embedding using the hidden state.\nalgo13.1 simple message passing neural network. Aggregate then update.\n\n\n13.2.3 Aggregation operators\n\n\n\n13.2.4 Update operators\n\n\n13.2.5 Node classification\n\n\n13.2.6 Edge classification\n\n\n13.2.7 Graph classification",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Graph neural networks</span>"
    ]
  },
  {
    "objectID": "13.html#general-graph-networks",
    "href": "13.html#general-graph-networks",
    "title": "13  Graph neural networks",
    "section": "13.3 General graph networks",
    "text": "13.3 General graph networks\n\n13.3.1 Graph attention networks\n\n\n13.3.2 Edge embedding\n\n\n13.3.3 Graph embedding\nAlgo13.2: general graph network training scheme.\n\nupdate edge embeddings\naggregate node info\nupdate node embeddings\nupdate graph embedding.\n\n  \n\n\n13.3.4 Over-smoothing\n\n\n13.3.5 Regularisation\n\n\n13.3.6 Geometric deep learning",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Graph neural networks</span>"
    ]
  },
  {
    "objectID": "14.html",
    "href": "14.html",
    "title": "14  Sampling",
    "section": "",
    "text": "14.1 basic sampling algorithms",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "14.html#basic-sampling-algorithms",
    "href": "14.html#basic-sampling-algorithms",
    "title": "14  Sampling",
    "section": "",
    "text": "14.1.1 expectations\n\nIn Bayesian inference the single most important thing is to calculate function expectations. Well actually maybe the only important thing. The same can be said anywhere in statistical inference or machine learning that probability distributions are involved.\n\n\n14.1.2 standard distributions\n\ngeometric interpretation of the transformation method for generating non-uniform distributed random numbers. h is the indefinite integral of the desired distribution. we sample uniformly on \\([0, 1]\\), then transform it using \\(y=h^{-1}(z)\\), then y is distributed according to p(y).\nto do this we need to 1) know h; 2) be able to inverse it.\n\nthe Box-Muller method for generating Gaussian distributed random numbers by generating samples from a uniform distribution inside the unit circle\n\n\n14.1.3 rejection sampling\n \n\n\n14.1.4 adaptive rejection sampling\n \n\n\n14.1.5 importance sampling\n\n\n\n14.1.6 sampling-importance-resampling (SIR)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "14.html#mcmc",
    "href": "14.html#mcmc",
    "title": "14  Sampling",
    "section": "14.2 MCMC",
    "text": "14.2 MCMC\nMCMC algorithms construct Markov chains that are designed to have the target probability distribution as their stationary distribution. The chain progresses through states (sample points) where each state depends only on the previous state (Markov property).\nThe theory of Markov chains underpins MCMC.\nThe Metropolis Algorithm is one of the earliest and simplest MCMC methods. The Metropolis-Hastings Algorithm is an extension of the Metropolis algorithm that allows for asymmetric proposal distributions.\nGibbs Sampling is a variant of the Metropolis-Hastings algorithm used when it’s easier to sample from the conditional distributions of a multivariate distribution than from the full joint distribution.\nAncestral Sampling is used primarily in directed graphical models (DGM).\n\n14.2.1 the Metropolis algorithm\n\n\n\n14.2.2 Markov chains\n\n\n14.2.3 the Metropolis-Hastings algorithm\n\n\n\n14.2.4 Gibbs sampling\n \n\n\n14.2.5 ancestral sampling",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "14.html#langevin-sampling",
    "href": "14.html#langevin-sampling",
    "title": "14  Sampling",
    "section": "14.3 Langevin sampling",
    "text": "14.3 Langevin sampling\nLangevin dynamics is used to calculate the gradients.\nEnergy based models convert an energy function into a probability density function, and normalise it using the integral over x.\nmaximising logp turns out to be balance data and model.\n\ntraining an energy-based model by maximizing the likelihood.\n\\[\n\\nabla_{\\mathbf{w}}\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{D}}[ \\ln p(\\mathbf{x}|\\mathbf{w}) ] = -\\mathbb{E}_{\\mathbf{x} \\sim p\\mathcal{D}}[ \\nabla_{\\mathbf{w}}E(\\mathbf{x},\\mathbf{w}) ] + \\mathbb{E}_{\\mathbf{x} \\sim p\\mathcal{M}(\\mathbf{x})}[ \\nabla_{\\mathbf{w}}E(\\mathbf{x},\\mathbf{w}) ].\n\\]\nIncreasing the expected data likelihood pushes the energy function up at points corresponding to samples from the model (current model belief) and pushes it down at points corresponding to samples from the data set (true data dist)\n\n14.3.1 energy-based models\nwe first define the density of one datum \\[\np(\\mathbf{x}|\\mathbf{w}) = \\frac{1}{Z(\\mathbf{w})} \\exp\\{-E(\\mathbf{x}, \\mathbf{w})\\}\n\\]\nthe normalizer just integrate over the whole space \\[\nZ(\\mathbf{w}) = \\int \\exp\\{-E(\\mathbf{x}, \\mathbf{w})\\} \\, d\\mathbf{x}\n\\]\nthen the log likelihood of a whole data set \\[ \\ln p(\\mathcal{D}|\\mathbf{w}) = -\\sum_{n=1}^{N} E(\\mathbf{x}_n, \\mathbf{w}) - N \\ln Z(\\mathbf{w}) \\]\nthis will be used as the optimization target.\n\n\n14.3.2 maximising the likelihood\nto optimise we need the gradient (to be zero) \\[\n\\nabla_{\\mathbf{w}} \\ln p(\\mathbf{x}|\\mathbf{w}) = -\\nabla_{\\mathbf{w}} E(\\mathbf{x}, \\mathbf{w}) - \\nabla_{\\mathbf{w}} \\ln Z(\\mathbf{w})\n\\]\naverage over the dataset \\[\n\\mathbb{E}_{\\mathbf{x} \\sim p_{\\mathcal{D}}}[\\nabla_{\\mathbf{w}} \\ln p(\\mathbf{x}|\\mathbf{w})] = -\\mathbb{E}_{\\mathbf{x} \\sim p_{\\mathcal{D}}}[\\nabla_{\\mathbf{w}}E(\\mathbf{x}, \\mathbf{w})] - \\nabla_{\\mathbf{w}} \\ln Z(\\mathbf{w})\n\\]\nand the gradient for the normalizing term can be expressed as \\[\n-\\nabla_{\\mathbf{w}} \\ln Z(\\mathbf{w}) = \\int \\nabla_{\\mathbf{w}}E(\\mathbf{x}, \\mathbf{w}) p(\\mathbf{x}|\\mathbf{w}) \\, d\\mathbf{x}\n\\]\nthe derivative of a log of an integral involves a “trick” where we differentiate the integrand and normalize it by the integral itself, essentially creating an expected value of the gradient of the energy function under the distribution defined by the normalized exponential of the negative energy, which is essentially p(x|w).\nso the gradient for a dataset ends up to be \\[\n\\nabla_{\\mathbf{w}}\\mathbb{E}_{x \\sim p_{\\mathcal{D}}}[\\ln p(\\mathbf{x}|\\mathbf{w})] = -\\mathbb{E}_{x \\sim p_{\\mathcal{D}}}[\\nabla_{\\mathbf{w}}E(\\mathbf{x}, \\mathbf{w})] + \\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x}|\\mathbf{w})}[\\nabla_{\\mathbf{w}}E(\\mathbf{x}, \\mathbf{w})]\n\\]\nit’s the gradient of the energy function, averaged over the data and the (current) model, in opposite directions.\n\n\n14.3.3 Langevin dynamics\nthe expectation wrt the data distribution is easy, we can simply use bootstrapping.\nthe expectation wrt the model distribution, however, is significantly more difficult, because although we can evaluate the current energy level, we cannot evaluate the normalizing constant.\nHowever, there exist a sampling algorithm, stochastic gradient Langevin dynamics sampling, which can sample the distribution using only the score of the logp wrt the data vectors, not the model parameters, and thus bypassing the problem. To see this, we first define the score function\n\\[\ns(\\mathbf{x}, \\mathbf{w}) = \\nabla_{\\mathbf{x}} \\ln p(\\mathbf{x}|\\mathbf{w})\n\\]\nsubstitue in the definition of logp and we have\n\\[\n\\nabla_{\\mathbf{x}} \\ln p(\\mathbf{x}|\\mathbf{w}) = \\nabla_{\\mathbf{x}} \\ln \\frac{\\exp(-E(\\mathbf{x},\\mathbf{w}))}{Z(\\mathbf{w})} \\\\\n&= \\nabla_{\\mathbf{x}} [-E(\\mathbf{x},\\mathbf{w})] - \\nabla_{\\mathbf{x}} \\ln Z(\\mathbf{w}) \\\\\n&= -\\nabla_{\\mathbf{x}} E(\\mathbf{x},\\mathbf{w})\n\\]\nsince the gradient wrt the normalizing term is zero, we are left only with the energy function, of which we can calculate the gradient.\nThe Langevin update process is\n\\[\n\\mathbf{x}^{(\\tau+1)} = \\mathbf{x}^{(\\tau)} + \\eta \\nabla_{\\mathbf{x}} \\ln p(\\mathbf{x}^{(\\tau)}|\\mathbf{w}) + \\sqrt{2 \\eta} \\epsilon^{(\\tau)}, \\quad \\tau \\in \\{1, \\ldots, T\\}\n\\]\nEach \\(T\\) steps give us one sample, and to obtain N samples we need \\(NT\\) steps. And with these samples we’ll also be able to estimate the second expectation, and now we have all the means necessary for gradient update.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "15.html",
    "href": "15.html",
    "title": "15  Discrete latent variables",
    "section": "",
    "text": "15.1 K-means clustering\nupdate assignments (E) and centers (M) till convergence\nplot of the cost function after each E step (in blue) and M step (in red).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Discrete latent variables</span>"
    ]
  },
  {
    "objectID": "15.html#k-means-clustering",
    "href": "15.html#k-means-clustering",
    "title": "15  Discrete latent variables",
    "section": "",
    "text": "15.1.1 Image segmentation\napplying K-means to image segmentation, with varying Ks",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Discrete latent variables</span>"
    ]
  },
  {
    "objectID": "15.html#mixture-of-gaussian",
    "href": "15.html#mixture-of-gaussian",
    "title": "15  Discrete latent variables",
    "section": "15.2 Mixture of Gaussian",
    "text": "15.2 Mixture of Gaussian\ngraph representation of mixture model (VERY SIMPLE!)\n\n\\[\n\\ln p(\\mathbf{X}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sum_{n=1}^{N} \\ln \\left\\{ \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right\\}\n\\]\n\n15.2.1 Likelihood function\n\n\n15.2.2 Maximum likelihood",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Discrete latent variables</span>"
    ]
  },
  {
    "objectID": "15.html#expectation-maximisation",
    "href": "15.html#expectation-maximisation",
    "title": "15  Discrete latent variables",
    "section": "15.3 Expectation maximisation",
    "text": "15.3 Expectation maximisation\n\n15.3.1 Gaussian mixtures\n\n\n15.3.2 Relation to K-means\n\n\n15.3.3 Bernoulli mixtures",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Discrete latent variables</span>"
    ]
  },
  {
    "objectID": "15.html#evidence-lower-bond",
    "href": "15.html#evidence-lower-bond",
    "title": "15  Discrete latent variables",
    "section": "15.4 Evidence lower bond",
    "text": "15.4 Evidence lower bond\n\n15.4.1 EM revisited\n\n\n15.4.2 IID data\n\n\n15.4.3 Parameter priors\n\n\n15.4.4 Generalised EM\n\n\n15.4.5 Sequential EM",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Discrete latent variables</span>"
    ]
  },
  {
    "objectID": "16.html",
    "href": "16.html",
    "title": "16  Continuous latent variables",
    "section": "",
    "text": "16.1 Principle conponent analysis",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous latent variables</span>"
    ]
  },
  {
    "objectID": "16.html#principle-conponent-analysis",
    "href": "16.html#principle-conponent-analysis",
    "title": "16  Continuous latent variables",
    "section": "",
    "text": "16.1.1 Maximum variance formulation\n\n\n16.1.2 Minimum error formulation\n\n\n16.1.3 Data compression\n\n\n16.1.4 Data whitening\n\n\n16.1.5 High dimensional data",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous latent variables</span>"
    ]
  },
  {
    "objectID": "16.html#probabilistic-latent-variables",
    "href": "16.html#probabilistic-latent-variables",
    "title": "16  Continuous latent variables",
    "section": "16.2 Probabilistic latent variables",
    "text": "16.2 Probabilistic latent variables\n\n16.2.1 Generative method\n\n\n16.2.2 Likelihood function\n\n\n16.2.3 Maximum likelihood\n\n\n16.2.4 Factor analysis (FA)\n\n\n16.2.5 Independent component analysis (ICA)\n\n\n16.2.6 Kalman filters",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous latent variables</span>"
    ]
  },
  {
    "objectID": "16.html#evidence-lower-bound",
    "href": "16.html#evidence-lower-bound",
    "title": "16  Continuous latent variables",
    "section": "16.3 Evidence lower bound",
    "text": "16.3 Evidence lower bound\n\n16.3.1 Expectation maximisation\n\n\n16.3.2 EM for PCA\n\n\n16.3.3 EM for FA",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous latent variables</span>"
    ]
  },
  {
    "objectID": "16.html#nonlinear-latent-variable-models",
    "href": "16.html#nonlinear-latent-variable-models",
    "title": "16  Continuous latent variables",
    "section": "16.4 Nonlinear latent variable models",
    "text": "16.4 Nonlinear latent variable models\n\n16.4.1 Nonlinear manifolds\n\n\n16.4.2 Likelihood function\n\n\n16.4.3 Discrete data\n\n\n16.4.4 Four approaches to generative modeling",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Continuous latent variables</span>"
    ]
  },
  {
    "objectID": "17.html",
    "href": "17.html",
    "title": "17  Generative adversarial networks",
    "section": "",
    "text": "17.1 Adversarial training",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Generative adversarial networks</span>"
    ]
  },
  {
    "objectID": "17.html#adversarial-training",
    "href": "17.html#adversarial-training",
    "title": "17  Generative adversarial networks",
    "section": "",
    "text": "17.1.1 Loss functions\n\n\n17.1.2 GAN training in practice",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Generative adversarial networks</span>"
    ]
  },
  {
    "objectID": "17.html#image-gans",
    "href": "17.html#image-gans",
    "title": "17  Generative adversarial networks",
    "section": "17.2 Image GANs",
    "text": "17.2 Image GANs\n\n17.2.1 CycleGAN",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Generative adversarial networks</span>"
    ]
  },
  {
    "objectID": "19.html",
    "href": "19.html",
    "title": "19  Autoencoders",
    "section": "",
    "text": "19.1 Deterministic autoencoders",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autoencoders</span>"
    ]
  },
  {
    "objectID": "19.html#deterministic-autoencoders",
    "href": "19.html#deterministic-autoencoders",
    "title": "19  Autoencoders",
    "section": "",
    "text": "19.1.1 Linear AEs\n\n\n19.1.2 Deep AEs\n\n\n19.1.3 Sparse AEs\n\n\n19.1.4 Denoising AEs\n\n\n19.1.5 Masked AEs",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autoencoders</span>"
    ]
  },
  {
    "objectID": "19.html#variational-autoencoders",
    "href": "19.html#variational-autoencoders",
    "title": "19  Autoencoders",
    "section": "19.2 Variational autoencoders",
    "text": "19.2 Variational autoencoders\n\n19.2.1 Amortized inference\n\n\n19.2.2 The reparameterization trick",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autoencoders</span>"
    ]
  }
]