---
title: structured distributions
---

no algorithms, 32 plots

## Graphical models

"Graphical models" is a misleading term.  The full name should be "Graphical representations of statistical models", or "independence diagrams of joint probability distributions".  Anyway, just like in life, we take shortcuts here and there to make things easier, while actually make them much harder in the end.

This section is an essential discussion of graphical models, we'll cover directed graphs and their factorizations; binary and Gaussian variables, their model parameters, and how graph structure affects model complexity; using binary classification as example, we also introduce the different components of the complete graph representations; finally we introduce Bayesian inference as inverse graphical models.

### Directed graphs

![](./img/DLFC/Chapter-11/Figure_1.png)
![](./img/DLFC/Chapter-11/Figure_2.png)
### Factorization

![](./img/DLFC/Chapter-11/Figure_3_a.png)
![](./img/DLFC/Chapter-11/Figure_3_b.png)
![](./img/DLFC/Chapter-11/Figure_4.png)
![](./img/DLFC/Chapter-11/Figure_5.png)
### Discrete variables

![](./img/DLFC/Chapter-11/Figure_6.png)
### Gaussian variables

![](./img/DLFC/Chapter-11/Figure_7.png)
### Binary classifier

![](./img/DLFC/Chapter-11/Figure_8.png)
![](./img/DLFC/Chapter-11/Figure_9.png)
### Parameters and observations

observed variables, latent variables, deterministic parameters

There are also two types of latent variables. The first is latent "variables", which are the latent variables in the traditional sense of machine learning. There are also model parameters, which are paremeters used to parameterize the observed and latent variables. However, in probabilistic modeling, and in their graphical representation, and in their Bayesian inference, the two kinds of latent variables are essentially treated the same.

![](./img/DLFC/Chapter-11/Figure_10.png)
![](./img/DLFC/Chapter-11/Figure_11.png)
![](./img/DLFC/Chapter-11/Figure_12.png)
### Bayes' theorem

![](./img/DLFC/Chapter-11/Figure_13_a.png)
![](./img/DLFC/Chapter-11/Figure_13_b.png)
![](./img/DLFC/Chapter-11/Figure_13_c.png)
## Conditional independence

To say a is conditionally (on c) independent of b is to say $p(a|b,c) = p(a|c)$. 
That is, once you are conditioning on c, knowing b doesn't give you any additional information about a.
Applying this to the product rule $p(a,b|c) = p(a|b,c) p(b|c)$ we get $p(a,b|c) = p(a|c) p(b|c)$. 

As we can see, conditional independence is a purely mathematical concept, and can always be verified using a bit of algebraic calculation.
However, with the help of graphical models we can directly read off the conditional independence properties without having to do any such tedious calculation.

### Three example graphs

![](./img/DLFC/Chapter-11/Figure_14.png)
![](./img/DLFC/Chapter-11/Figure_15.png)
![](./img/DLFC/Chapter-11/Figure_16.png)
![](./img/DLFC/Chapter-11/Figure_17.png)
![](./img/DLFC/Chapter-11/Figure_18.png)
![](./img/DLFC/Chapter-11/Figure_19.png)

### Explaining away

![](./img/DLFC/Chapter-11/Figure_20.png)

### D-separation

![](./img/DLFC/Chapter-11/Figure_21.png)

### Naive Bayes

![](./img/DLFC/Chapter-11/Figure_22.png)

### Generative models

![](./img/DLFC/Chapter-11/Figure_24.png)

### Markov blanket

![](./img/DLFC/Chapter-11/Figure_25.png)

### Graph as filters

## Sequence models

### Hidden variables
