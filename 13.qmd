---
title: "Graph neural networks"
---

![](./img/DLFC/Chapter-13/Figure_1.png)

## Machine learning on graphs

### Graph properties

### Adjacency matrix

![](./img/DLFC/Chapter-13/Figure_2_a.png)
![](./img/DLFC/Chapter-13/Figure_2_b.png)
![](./img/DLFC/Chapter-13/Figure_2_c.png)

### Permutation equivariance

The ordering of the nodes are often arbitrary.

## Neural message passing

### Convolutional filters

Like in vision CNN models, the receptive field is quite small and fixed. Vision models move the receptive field across the image, while in graph models, the receptive field is gradually enlarged using multiple layers of message passing.

![](./img/DLFC/Chapter-13/Figure_3_a.png)
![](./img/DLFC/Chapter-13/Figure_3_b.png)

### Graph convolution networks (GCN)

Each node has an associated hidden variable that stores the aggragated info from all its neighbours.
Each layer of the network updates the node embedding using the hidden state.

algo13.1 simple message passing neural network. Aggregate then update.

### Aggregation operators

![](./img/DLFC/Chapter-13/Figure_4.png)

### Update operators

### Node classification

### Edge classification

### Graph classification

## General graph networks

### Graph attention networks

### Edge embedding

### Graph embedding

Algo13.2: general graph network training scheme.

1. update edge embeddings
2. aggregate node info
3. update node embeddings
4. update graph embedding.

![](./img/DLFC/Chapter-13/Figure_5_a.png)
![](./img/DLFC/Chapter-13/Figure_5_b.png)
![](./img/DLFC/Chapter-13/Figure_5_c.png)

### Over-smoothing

### Regularisation

### Geometric deep learning
