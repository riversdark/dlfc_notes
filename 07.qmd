---
title: "Gradient Descent"
jupyter: "torch"
---

Once we have a neural network, the next question is how to train it. (Well maybe not so straightforward. Often model building and training is an iterative process, and what we can train directly determines what we will build.)

To train a network we'd also need a loss function, which measures how well the network performs. Since model performance can be measured in different ways, for different purposes we might need different loss functions. But since many network outcomes are modeled using various probability distributions, the most common one we use is -logp, the negative log data likelihood (often summed over a batch of data).

And although stochastic gradient descent (and its variants and derivatives) are the most common method to train a network, it's actually not the only one. For example the optimization method might not be stochastic (using the whole dataset at once), or it might not be gradient based (gradient free optimization, Bayesian optimization, etc), but SGD seems to be the one that scales best. For SGD to work we of course need the gradient, and to have gradient we'd need the loss function to be smooth and differentiable, and this necessitates some structural constraints on the network we are building, and the loss function we are using. As we will see later on, residual connections are a widely used technique to ensure smoothness of the loss function.

There have been many modifications to SGD, which improve the convergence properties of the optimizer, by studying the local curvature of the loss function. Most of these methods are actually heuristic, but some of them work quite well, especially the adaptive moments method (commonly known as Adam).

This chapter also covers some common normalization methods, including data normalisation, batch normalisation, and layer normalisation. This is a typical case of modifying the network architecture to improve training, showing that it's not always easy to separate modeling building from model training.

8 plots, 4 algorithms

## Error surfaces 

only by understanding the topology of the error surface can we know how to navigate it.
However, the error surface can be extremely complicated: most of what we do are based on quadratic approximation.

around local minimum, Jacobian is zero and Hessian positive definite (all eigenvalues positive)


![](./img/DLFC/Chapter-7/Figure_1.png)

error function, local minimum A, global minimum B, and gradient at C

### Local quadratic approximation

![](./img/DLFC/Chapter-7/Figure_2.png)

## Gradient descent optimisation 

Simply put, gradient descent initialize the model parameters somewhere, inspect the local curvature of the loss function using gradient information, and modify the parameters using this information. Most of our attention will be devoted to how to get better **local** curvature information, but the starting point (initialization) is also quite important, exactly because of the **local** nature of gradient descent.

algo7.1 SGD
algo7.2 mini-batch SGD


$$
\mathbf{w}^{(\tau)} = \mathbf{w}^{(\tau-1)} + \Delta \mathbf{w}^{(\tau-1)} 
$$

### Use of gradient information

### Batch gradient descent

### Stochastic gradient descent

### Mini-batches

### Parameter initialisation

## Convergence 

![](./img/DLFC/Chapter-7/Figure_3.png)

Local gradient vector is perpendicular to local contour, thus not pointing towards the local minimum. This causes the zig-zag pattern in the optimization path.


### Momentum

Momentum is a method to accelerate SGD by adding a fraction of the previous gradient to the current gradient, but how much of an effect this has depends on the curvature of the loss function. When the curvature is low, consecutive gradients are similar in direction, and thus accumulate to speed up the exploration. When the curvature is high, the gradients are different in direction and thus cancel each other out, leading to dampened oscillations. (Either way the effect seems to be positive.)

![](./img/DLFC/Chapter-7/Figure_4.png)
![](./img/DLFC/Chapter-7/Figure_5.png)

And the modified optimization path should look something like this:

![](./img/DLFC/Chapter-7/Figure_6.png)

Nesterov momentum is similar, but it effectively introduces a 1/2 step, first computing a step using the previous momentum, then calculate the gradient at that point, and then update the momentum again.

### Learning rate schedule

### RMSProp and Adam

## Normalisation 

Remember that the log likelihood is actually a function of two sets of inputs: the network parameters and the data. Given the log likelihood function, momentum can help to regularize the search path in parameter space. But we can also normalize the data, so that the error surface itself is more regularized, which helps to ease the exploration. All the normalization techniques discussed here attempt to help in this regard, but adapted to different scenarios. We can apply data normalization to the input data; and batch normalization to the hidden layer inputs. And when getting batches of data is difficult, we might also settle for layer normalization. Either way, the overarching goal is to regularize the data flow so to regularize the optimization path.

The gopher(2022) paper also introduced a RMSNorm normalisation method.

### Data normalisation

![](./img/DLFC/Chapter-7/Figure_7.png)

### Batch normalisation

![](./img/DLFC/Chapter-7/Figure_8_a.png)

### Layer normalisation

![](./img/DLFC/Chapter-7/Figure_8_b.png)