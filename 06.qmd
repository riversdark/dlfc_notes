---
title: "Deep Neural Networks"
---

According to the universal approximation theory, a two-layer network can already approximate all possible smooth functions. If so, why go deeper? This is the question we want to answer in this chapter. 

This chapter is largely theoretical, concrete implementations of specific model architectures are deferred to later chapters.

## Limitations of fixed basis functions

### The curse of dimensionality

### High dimensional spaces

![](./img/DLFC/Chapter-6/Figure_5.png)

In high dimensional spaces, consider a local mode: the probability density decreases further away from the mode, while the volume of the sphere increases exponentially with the number of dimensions. As the combined consequence of these two contradicting effects, the probability mass will concentrate somewhere in between. (The typical set.)

### Data manifolds

digits

### Data-dependent basis functions

kernel methods

## Multilayer networks

![](./img/DLFC/Chapter-6/Figure_9.png)

### Parameter matrices

### Universal approximation

with examples

### Hidden unit activation functions

softmax

### weight-space symmetries

## Deep networks

### Hierarchical representations

### Distributed representations

### Representation learning

### Transfer learning

### Contrastive learning

### General network architectures

### Tensors

## Error functions

### Regression

### Binary classification

### Multiclass classification

## Mixture density networks

This is in fact a mixture model, where each component is parameterized by a neural network.

![](./img/DLFC/Chapter-6/Figure_17_a.png)
![](./img/DLFC/Chapter-6/Figure_17_b.png)

forward vs inverse problem. A single NN can deal with one but not the other (see red fitted curve)

### Robot kinematics example

### Conditional mixture distributions

### Gradient optimization

### Predictive distribution
